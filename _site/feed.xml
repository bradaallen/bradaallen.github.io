<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-01T12:00:46-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Perpetually Learning</title><subtitle>Brad Allen's personal blog. I love building things (and learning how to build new things). Preferably those that scale. Keywords: Product Management, Data Science, Data Engineering, Python, Stanford, Stanford GSB, Penn State, Pennsylvania, TFA, Teach For America, Bain, Embrace, Aclima, Peloton Technology, Mechatronics, C+
</subtitle><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/jekyll/update/welcome-to-jekyll/" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2020-03-11T17:28:12-07:00</published><updated>2020-03-11T17:28:12-07:00</updated><id>http://localhost:4000/jekyll/update/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/jekyll/update/welcome-to-jekyll/">&lt;p&gt;You’ll find this post in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated. To add new posts, simply add a file in the _posts directory that follows the convention YYYY-MM-DD-name-of-post.ext and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works. Jekyll also offers powerful support for code snippets: def print_hi(name) puts &quot;Hi, #{name}&quot; end print_hi('Tom') #=&amp;gt; prints 'Hi, Tom' to STDOUT. Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk.</summary></entry><entry><title type="html">Is Your Customer Journey Set Up for Success? Part II.</title><link href="http://localhost:4000/customer-journey-success-part-2/" rel="alternate" type="text/html" title="Is Your Customer Journey Set Up for Success? Part II." /><published>2017-07-10T00:00:00-07:00</published><updated>2017-07-10T00:00:00-07:00</updated><id>http://localhost:4000/customer-journey-success-part-2</id><content type="html" xml:base="http://localhost:4000/customer-journey-success-part-2/">&lt;p&gt;&lt;em&gt;This article was written during my time at SVDS. The article was first published on the &lt;a href=&quot;https://www.svds.com/customer-journey-set-success/&quot;&gt;SVDS Blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In the &lt;a href=&quot;https://bradaallen.github.io/customer-journey-success-part-1/&quot;&gt;introductory post&lt;/a&gt;, we walked through some examples of how SVDS has seen data capabilities determine the success of customer journey initiatives for our clients. In this post, we offer guidance on the data-related initiatives that you can start today to begin fostering closer ties with your customers—regardless of where you currently are in your specific state of development.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;customer-journeys-allow-for-a-holistic-experience&quot;&gt;Customer journeys allow for a holistic experience&lt;/h3&gt;

&lt;p&gt;A senior marketer’s ability to find and create valuable experiences for customers has grown dramatically in recent years. Beyond the traditional responsibilities of brand and creative management, senior marketers (such as CMOs, Brand Managers, and Product Marketing Managers) now use analytics to track customer interactions, measure the quality of engagement, and determine the effectiveness of an enormous range of different marketing tactics.&lt;/p&gt;

&lt;p&gt;Marketers often map out a &lt;a href=&quot;https://hbr.org/2015/11/competing-on-customer-journeys&quot;&gt;“customer journey”&lt;/a&gt; in order to manage successful engagements. The customer journey is the complete sum of experiences that your customers go through when interacting with your company and brand—mapping out these interactions gives you a holistic view of how customers engage with your company. While many marketers focus on developing positive interactions, a customer journey is a plan that focuses on how a series of engagements can generate momentum from awareness, to sale, to ongoing loyalty and advocacy.&lt;/p&gt;

&lt;h3 id=&quot;creating-customer-journey-opportunities-with-data&quot;&gt;Creating customer journey opportunities with data&lt;/h3&gt;

&lt;p&gt;At SVDS, we have seen leaders use data to drive more useful customer engagements—first and foremost by recognizing their need to embrace change. There is a big shift taking place, and that shift will become the new normal.&lt;/p&gt;

&lt;p&gt;You should be trying to learn quickly, and fail fast. Companies that are able make the best use of their data and infrastructure earlier than their competition are at an advantage—both with regard to increased customer loyalty and improvements in new product development.&lt;/p&gt;

&lt;p&gt;In a &lt;a href=&quot;https://bradaallen.github.io/customer-journey-success-part-1/&quot;&gt;November 2015 Harvard Business Review article&lt;/a&gt; on the customer journey, the authors stated that, “Best practitioners aim not just to improve the existing journey but to expand it, adding useful steps or features.” As mentioned in the examples earlier in this post, we have seen the same trend: clients who can harness their data to create effective customer experiences often make further investments toward developing their capabilities.&lt;/p&gt;

&lt;p&gt;There are data-related initiatives that you can begin pursuing today to develop stronger relationships with your customers. Make an honest assessment of where you stand now, and find yourself in the sections below.&lt;/p&gt;

&lt;h4 id=&quot;if-you-are-starting-from-scratch&quot;&gt;If you are starting from scratch:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Common Challenges&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Redefining decision-making based on insights&lt;/li&gt;
      &lt;li&gt;Identifying single points of ownership&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Common Solutions&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Focus on top-down buy-in. Without recognition from leadership, data initiatives will struggle to get relevance with business users and may be piecemeal efforts, diminishing the value of investment.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;if-you-are-performing-early-project-identification&quot;&gt;If you are performing early project identification:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Common Challenges&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Mapping out customer journey initiatives&lt;/li&gt;
      &lt;li&gt;Identifying single points of ownership&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Common Solutions&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Get alignment on the full picture. It is important to be able to articulate the “full view” of the customer experience as it has a large effect on decision making. For example, only tracking successful interactions would lead to very different conclusions than understanding users that “turn away.”&lt;/li&gt;
      &lt;li&gt;Plan for iteration. It often takes time to understand where your map does and does not match your customers’ realities.&lt;/li&gt;
      &lt;li&gt;Start small. Change can be incremental—look for low-hanging fruit.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;if-you-are-establishing-customer-visibility&quot;&gt;If you are establishing customer visibility:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Common Challenges&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Data integration&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Common Solutions&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Prioritize data collection in tandem with journey mapping. In instrumentation, it is important to know what behaviors you can collect directly from customers and what behaviors you have to infer based on their actions. This influences what additional data sources you include to support decisions you make for the business.&lt;/li&gt;
      &lt;li&gt;Seek to reuse and extend data services. Developing known, validated, and consistent data assets for your business increases their utility and dramatically improves trust in the developed insights across the organization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;if-you-are-optimizing-for-growth&quot;&gt;If you are optimizing for growth:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Common Challenges&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Personalization and automation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Common Solutions&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Enable advanced analytics. Optimize for automation to create feedback loops and self-learning capabilities that make it easy to identify and capitalize on growth opportunities.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What steps are you taking to strengthen your customer engagement strategies through better use of data?&lt;/p&gt;</content><author><name></name></author><summary type="html">Practical recommendations for any level of sophistication.</summary></entry><entry><title type="html">Is Your Customer Journey Set Up for Success? Part I.</title><link href="http://localhost:4000/customer-journey-success-part-1/" rel="alternate" type="text/html" title="Is Your Customer Journey Set Up for Success? Part I." /><published>2017-07-03T00:00:00-07:00</published><updated>2017-07-03T00:00:00-07:00</updated><id>http://localhost:4000/customer-journey-success-part-1</id><content type="html" xml:base="http://localhost:4000/customer-journey-success-part-1/">&lt;p&gt;&lt;em&gt;This article was written during my time at SVDS. The article was first published on the &lt;a href=&quot;https://www.svds.com/customer-journey-set-success/&quot;&gt;SVDS Blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;customer-journeys-allow-for-a-holistic-experience&quot;&gt;Customer journeys allow for a holistic experience&lt;/h3&gt;

&lt;p&gt;A senior marketer’s ability to find and create valuable experiences for customers has grown dramatically in recent years. Beyond the traditional responsibilities of brand and creative management, senior marketers (such as CMOs, Brand Managers, and Product Marketing Managers) now use analytics to track customer interactions, measure the quality of engagement, and determine the effectiveness of an enormous range of different marketing tactics.&lt;/p&gt;

&lt;p&gt;Marketers often map out a &lt;a href=&quot;https://www.forrester.com/Customer-Journey&quot;&gt;“customer journey”&lt;/a&gt; in order to manage successful engagements. The customer journey is the complete sum of experiences that your customers go through when interacting with your company and brand—mapping out these interactions gives you a holistic view of how customers engage with your company. While many marketers focus on developing positive interactions, a customer journey is a plan that focuses on how a series of engagements can generate momentum from awareness, to sale, to ongoing loyalty and advocacy.&lt;/p&gt;

&lt;p&gt;According to a &lt;a href=&quot;https://www.salesforce.com/blog/2014/11/new-report-salesforce-marketing-cloud-linkedin-state-marketing-leadership.html&quot;&gt;Salesforce report&lt;/a&gt;, nearly all senior level marketers agree that a comprehensive journey map is absolutely critical or very important to their business. At the same time, the report mentions that this map has largely been an aspiration for marketers—in part due to siloed business teams and a disjointed view of customer data, only 29% of enterprise companies would rate themselves as very effective or effective at creating a cohesive journey.&lt;/p&gt;

&lt;p&gt;Senior marketers should take the responsibility for this challenge head on; to be successful in creating a useful map, you will also need to be the leader of the technical and analytical development of their teams. You should have an intuition for how data can enhance, track, and articulate the customer experience—as this intuition creates new possibilities for the type of relationships companies can have with their customers.&lt;/p&gt;

&lt;p&gt;In this first post, we’ll walk through some examples of how we have seen data capabilities determine the success of customer journey initiatives for our clients. In &lt;a href=&quot;https://bradaallen.github.io/customer-journey-success-part-2/&quot;&gt;subsequent posts&lt;/a&gt;, we’ll also offer guidance on the data-related initiatives that you can start today to begin fostering closer ties with your customers—regardless of where you currently are in your specific state of development.&lt;/p&gt;

&lt;h3 id=&quot;what-can-your-data-do-for-you&quot;&gt;What can your data do for you?&lt;/h3&gt;

&lt;p&gt;We’ve seen data play various roles in creating strong customer engagements. Here’s a look at just a few.&lt;/p&gt;

&lt;h4 id=&quot;optimize-against-competing-outcomes-using-effectively-stitched-data-social-gaming&quot;&gt;Optimize against competing outcomes using effectively stitched data (Social Gaming)&lt;/h4&gt;

&lt;p&gt;For many businesses, legacy and heterogeneous systems are a challenge for creating an integrated customer experience—the data is often structured with a narrow lens on a specific product or domain. We worked with a social gaming company facing this challenge: they wanted to create a cohesive customer experience across all their games by extending the preferential treatment that loyal customers receive for their favorite games to new games on the platform.&lt;/p&gt;

&lt;p&gt;On a per-game basis, our client was technically sophisticated—they could build out events, correlate performance with targeted marketing strategies, and articulate the effectiveness of different campaigns. However, this sophistication fell apart at the organization-wide level. After several acquisitions and the use of third party licensing for games, the company found itself with a broken analytical architecture—each game optimized for itself, but there was a lot of opportunity in optimizing across the business. By supporting the integration of different data from different silos, the new architecture enabled the company to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Increase customer satisfaction by rewarding loyalty.&lt;/em&gt; By not being able to establish a single view of their customers, our client was continually losing opportunities to tailor experiences for their customers. For example, high-paying customers that had “preferred” status for certain games would return to being “unknown” when they started playing new games. By helping the client carry customer status across games, players will have greater satisfaction and loyalty.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Improve game development by understanding interaction patterns.&lt;/em&gt; Some of the most important metrics in gaming—e.g., alliances and teaming—are challenging to measure. Breaking down silos allowed the client to utilize novel sources of data, like gaming chats, to articulate a “web of influence” and its role in engagement and profitability.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By reframing the customer journey as an experience that transcends individual games—and developing a supporting data architecture—our client was able to both develop an engine for growth and improve profitability on an individual basis, by both reducing the acquisition cost (UAC) of customers and increasing their lifetime value (LTV).&lt;/p&gt;

&lt;h4 id=&quot;redefine-your-brand-and-product-suite-with-a-more-modern-architecture-digital-entertainment&quot;&gt;Redefine your brand and product suite with a more modern architecture (Digital Entertainment)&lt;/h4&gt;

&lt;p&gt;Take a look at the data sources you’re using in your marketing efforts and you may find some unexpected insights. For example, is there untapped value in your existing customer relationships? A digital entertainment company wanted to develop a modern database architecture that would allow them to understand user consumption at both a customer and population-level—at a microsecond granularity.
Redesigning the client’s architecture to identify and articulate the customer’s consumption patterns ultimately gave the client vastly more usable data about their customers, which led to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;New customer offerings.&lt;/em&gt; Our client used these new capabilities to develop more effective cross-selling opportunities and to develop products that provide guidance to content providers.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Improved strategic decision making.&lt;/em&gt; This view of consumption informed large strategic bets for the organization—for example, the decision to give away existing products for free in return for increased engagement.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The client had always had tremendous potential to understand customer engagement and consumption patterns—they were a portal for millions of users. However, their existing platform was limited by its underlying technology and a myopic view of the role of data. In their original product offering, the collected data was not necessarily perceived to have inherent value. We find this type of oversight to be common for marketers beginning to build analytics within their teams.&lt;/p&gt;

&lt;p&gt;Note: This concept of “instrumentation”—the process of logging and tracking customer interactions—is important when creating an engagement plan. Instrumentation creates a more nuanced understanding of what customers find valuable about your products and services. For our client, this instrumentation influenced their entire business: from feature development, to sales, to pricing, and even to marketing copy about the efficacy of their existing product suite. Instrumentation is so important, in fact, that it is something we at SVDS specifically assess when considering the &lt;a href=&quot;https://www.svds.com/understanding-your-data-maturity/&quot;&gt;data maturity of a business&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;update-back-office-processes-to-redefine-customer-expectations-national-retailer&quot;&gt;Update back-office processes to redefine customer expectations (National Retailer)&lt;/h4&gt;

&lt;p&gt;Work that begins in the marketing department often extends to influence other parts of a business. In one example, we worked with a traditional brick and mortar retail client that was in the process of developing their digital presence.&lt;/p&gt;

&lt;p&gt;As their customers began to spend more time purchasing items online, our client realized that they would need new capabilities to support new types of interactions—for example, granularity in tracking inventory. When creating its online presence, our client found itself frequently selling under-stocked items from the website, and then had to follow them up with costly gift card apologies to disappointed customers.&lt;/p&gt;

&lt;p&gt;We helped the client gain an understanding of their inventory baseline and establish a real-time view of changes in supply and demand. This allowed the business as a whole to establish new customer relationships and a leaner efficiency:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Customer interaction with the brand.&lt;/em&gt; E-commerce has developed some great features to incent purchasing behavior—we have all experienced messages like, “There are only 3 items left in stock!” or “Order by 11:59pm Tuesday to get by Christmas.” Our client’s new modern inventory infrastructure allowed them to create similar features, increasing trust and confidence in the brand.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Reduced working capital.&lt;/em&gt; In the past, a granular level of detail—across stores, channels, and partners (e.g., third party sellers)—was not required from legacy operations. In developing a new system, our client was able to serve items that were stocked-out in their web distribution centers from stores in close proximity to the buyers. This allowed our client to reduce their buffer stock and protect against obsolescence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For our retail client, marketing’s strategic influence and digital leadership forced growth in other parts of the business that benefited customers and improved the company’s competitive position.&lt;/p&gt;

&lt;h3 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h3&gt;

&lt;p&gt;How is your company taking steps to strengthen their customer engagement strategies through better use of data?&lt;/p&gt;</content><author><name></name></author><summary type="html">Case studies from high performing transformations.</summary></entry><entry><title type="html">Technology is Only a Precondition for Success with Data</title><link href="http://localhost:4000/technology-isnt-everything/" rel="alternate" type="text/html" title="Technology is Only a Precondition for Success with Data" /><published>2017-05-15T00:00:00-07:00</published><updated>2017-05-15T00:00:00-07:00</updated><id>http://localhost:4000/technology-isnt-everything</id><content type="html" xml:base="http://localhost:4000/technology-isnt-everything/">&lt;p&gt;&lt;em&gt;This article was co-authored along with Scott Kurth, based on our experience at SVDS. The article was first published on the &lt;a href=&quot;https://www.svds.com/value-centered-data-maturity/&quot;&gt;SVDS Blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;success-with-data-involves-managing-strengths-and-weaknesses&quot;&gt;Success with data involves managing strengths and weaknesses&lt;/h3&gt;

&lt;p&gt;The great data rush is well and truly under way. Across virtually every industry, companies large and small are committing serious money to standing up their data infrastructure, beefing up capabilities, and hunting for the value hidden in their data—but often without a clear plan. No wonder that many of the business leaders we speak with suspect their initiatives are underperforming. The complaint we hear most frequently, regardless of industry, is that technology investments aren’t generating the expected returns.&lt;/p&gt;

&lt;p&gt;In a &lt;a href=&quot;https://www.svds.com/first-steps-strategy/&quot;&gt;previous post on data maturity&lt;/a&gt;, we discussed a company that was just embarking on a transformation: launching a new services business and building data capabilities to support that business. But what if you’re not starting from the beginning? What if you’ve already been embracing new technology, conducting pilots, and launching new analytical platforms? Recently, we were working with a Fortune 500 industrial company in the midst of developing software services to improve product R&amp;amp;D and enrich the customer experience.&lt;/p&gt;

&lt;p&gt;Their goal was to &lt;a href=&quot;https://svds.com/building-a-data-driven-culture/&quot;&gt;use data to empower decision makers&lt;/a&gt; across every part of the organization to make robust, data-driven choices. The company had great talent, technical vision, and infrastructure. &lt;strong&gt;Still, they weren’t generating the progress they would have liked at a rate they would have expected. What was wrong?&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;sometimes-the-business-can-hold-good-technology-behind&quot;&gt;Sometimes, the business can hold good technology behind&lt;/h3&gt;

&lt;p&gt;Working with our clinet to articulater their overall data maturity shined a light on areas requiring attention to get the most of their technology investments:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Missing links between projects and metrics:&lt;/em&gt; The initiative’s overall success was being measured by a single metric that they could only begin tracking in 2020—at the completion of the transformation. This led to significant uncertainty within project teams building new capabilities and platforms. Many teams were unsure where their analytical work fit in the big into the larger efforts and, more importantly, whether they were contributing to the overall success of the transformation.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Lack of cross-functional teams:&lt;/em&gt; The analytical infrastructure built by the engineering team was impressive, but was sorely underutilized. The data scientists had not been trained to use it and did not know how to access it. We heard from an analytics manager: “Seventy percent of my team’s time is spent on writing UDFs and Pig scripts to access data!” Creating teams that facilitated collaboration between engineers and data scientists was an opportunity for quick productivity gains with expensive talent.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Siloed business functions:&lt;/em&gt; Teams felt a lack of clear objectives that stemmed from communication and information sharing issues with other teams. For example, one team integral to product development described their view of the future as “a dusty window.” Business units on the consumption side of application development experienced very uneven usage of analytical tools. Strengthening these partnerships was crucial as the overarching project’s success relied specifically on strong analytical capabilities throughout the entire organization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In working together, we helped the industrial company link their data and analytical capabilities with their ultimate business objectives, allowing them to create the right metrics to truly understand their progress. We helped them improve their devops capabilities and better integrate their engineering and data science teams. Collectively, this helped them break down technical and organizational siloes that were hampering progress.&lt;/p&gt;

&lt;p&gt;Understanding the uneven maturity of their capabilities across people, process, and systems gave them the answers they needed to the question on everyone’s minds: How can we see real results faster? A view of the current state of maturity along with a clear roadmap for success creates a baseline and a way to measure progress.&lt;/p&gt;</content><author><name></name></author><summary type="html">Knowing what to do with it is what matters most.</summary></entry><entry><title type="html">Understanding Your Data Maturity</title><link href="http://localhost:4000/understanding-data-maturity/" rel="alternate" type="text/html" title="Understanding Your Data Maturity" /><published>2017-05-01T00:00:00-07:00</published><updated>2017-05-01T00:00:00-07:00</updated><id>http://localhost:4000/understanding-data-maturity</id><content type="html" xml:base="http://localhost:4000/understanding-data-maturity/">&lt;p&gt;&lt;em&gt;This article was co-authored along with Scott Kurth, based on our experience at SVDS. The article was first published on the &lt;a href=&quot;https://www.svds.com/understanding-your-data-maturity/&quot;&gt;SVDS Blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-effective-are-you-with-your-capabilities&quot;&gt;How &lt;em&gt;effective&lt;/em&gt; are you with your capabilities?&lt;/h3&gt;

&lt;p&gt;Businesses today are experiencing rapid change, inside and outside of Silicon Valley. Even coffee isn’t immune—just last year, Starbucks crossed the threshold of &lt;a href=&quot;https://www.bloomberg.com/news/articles/2016-03-30/starbucks-takes-its-pioneering-mobile-phone-app-to-grande-level&quot;&gt;more than 20% of transactions being done in their mobile application&lt;/a&gt;. Data is the currency of &lt;a href=&quot;https://svds.com/optimizing-your-digital-strategy/&quot;&gt;digital transformation&lt;/a&gt;, and data capabilities are the battlefield influencing market share and profitability.&lt;/p&gt;

&lt;p&gt;No two situations are the same, but at SVDS we have found one truism: making a data transformation successful requires much more than simply getting the technology right. Across a variety of industries and operations, we consistently find the influence of systems and people to be deciding factors. This is one of the reasons why we developed our &lt;a href=&quot;https://svds.com/what-are-you-doing-with-your-data/&quot;&gt;Data Maturity Model&lt;/a&gt; — a tool for understanding how well your data capabilities create value for your business, across people, process, and systems.&lt;/p&gt;

&lt;h3 id=&quot;data-maturity-in-practice&quot;&gt;Data maturity in practice&lt;/h3&gt;

&lt;p&gt;Data maturity is a useful tool for measuring the progress being made against your transformation. Recently, we were working with a multi-billion dollar industrial device company that was just beginning their Internet of Things (IoT) transformation: integrating software services with their physical devices. The vision was set and small experiments were being run throughout the organization—but the real work of building had not yet begun.&lt;/p&gt;

&lt;p&gt;The company’s overarching strategy made a lot of sense—building a higher-margin services business from the key position their devices play in their customers’ workflows. We were asked to help them develop a data strategy and accompanying architecture to make that possible. Fundamentally, they needed a plan to use analytics and device data to create new services their clients would value.&lt;/p&gt;

&lt;p&gt;While this company certainly had major technology investments in its future, some of the most urgent things necessary for this transformation to be successful had little to do with technology:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;New incentives to change required:&lt;/em&gt; Like many established product companies, the organization was structured around distinct, mature product lines. With very different P&amp;amp;Ls and incentives, we were told: “Careers are made within the business units, not the company.” There was an inherent skepticism for investing in unproven growth, especially if the effort was performed by a centralized function.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Lack of experience with data rights:&lt;/em&gt; At this time, a very small percentage of overall revenue was produced through software. To sidestep questions of license management, the default behavior was to “make stuff free so that we do not have to create licenses.”&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;New customer relationships (and approach to product development):&lt;/em&gt; Getting the most value from device data meant integrating it with some of the customers’ own data sources to provide greater context to end users. This meant expanding or revisiting customer relationships to embrace data sharing, requiring updates to processes in sales management (i.e., around channel or partnership agreements) and overarching application strategies (e.g., creating standards and cultivating ISV ecosystems).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In working together, we helped the industrial product company identify solutions for incentivizing technical investment. We identified policies required around data ownership, custody, and consent as a precursor to becoming an integrator and reseller in the supply chain of data services. We helped them update their architecture to support these services. It is not uncommon for an SVDS maturity assessment to reveal a broad set of opportunities.&lt;/p&gt;

&lt;p&gt;Ultimately, our client was equipped with a broader perspective on what they needed to launch their software business and a roadmap to build all aspects of their data capabilities, ensuring that their transformation would be more successful. A clear roadmap helped them fill in the gaps between here and there, illuminating the concrete steps they needed to achieve their vision. Although transformation does not have a fixed end point, milestones are important—change should be incremental enough that the organization can see progress.&lt;/p&gt;

&lt;p&gt;Although it won’t give you a full picture of your organization, these exercises are a good starting point to help you gain a better sense of your company’s data maturity and spark productive exchanges with your colleagues. Is your organization using data just for generating reports or focusing on use cases in mission-critical areas? Or is it putting data first in every business activity? Knowing where you stand will increase your understanding of your organization’s potential, provide a baseline for measuring progress, and give you a framework for thinking about your data operations and how they might compare with the competition’s and your industry’s as a whole.&lt;/p&gt;</content><author><name></name></author><summary type="html">First steps in a data strategy.</summary></entry><entry><title type="html">ML Practice: Exploring the Enron Set</title><link href="http://localhost:4000/udacity-ml-introduction/" rel="alternate" type="text/html" title="ML Practice: Exploring the Enron Set" /><published>2016-12-07T00:00:00-08:00</published><updated>2016-12-07T00:00:00-08:00</updated><id>http://localhost:4000/udacity-ml-introduction</id><content type="html" xml:base="http://localhost:4000/udacity-ml-introduction/">&lt;p&gt;&lt;em&gt;Note: If you would like to skip directly to my Jupyter notebook of this exercise, you can find it &lt;a href=&quot;https://github.com/brad-svds/udacity_practice/blob/master/poi_id.py.ipynb&quot;&gt;at this link&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;project-summary&quot;&gt;Project Summary&lt;/h2&gt;

&lt;p&gt;This project is the capstone for the &lt;a href=&quot;https://www.udacity.com/course/intro-to-machine-learning--ud120&quot;&gt;“Introduction to Machine Learning”&lt;/a&gt; course in the Udacity Machine Learning Engineering nanodegree. It involves using the “Enron corpus”—a body of information that was made publicly available following the Enron scandal in the early 2000s. It has all salary information and email communications. It is rare, as most companies keep these communications private. For CS folks, it is particularly valuable—it is a dataset that reveals common networks, patterns, and communications. This is a useful basis for developing and testing new models about how people interact in the workplace.&lt;/p&gt;

&lt;p&gt;The goal of project is to use a dataset built from the corpus to create a model that identifies “Persons of Interest” (POIs). A POI is someone whom we believe is a strong candidate for having committed fraudulent activities—for example, Jeffrey Skilling and Kenneth Lay. The dataset includes characteristics about the different individuals at Enron—for example, their known interactions with POIs (how many emails did they send or receive), the portion of their total compensation that was a bonus, etc. Doing some cursory analysis, we have 21 characteristics/columns we could use as “features” in our model. There are 146 individuals in the dataset, of whom 128 are NOT POIs, and 18 are POIs.&lt;/p&gt;

&lt;figure align=&quot;center&quot;&gt;
	&lt;img src=&quot;/images/skilling.jpg&quot; /&gt;
	&lt;figcaption&gt;This guy is definitely a &quot;POI&quot;.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The model we are trying to create is an algorithm that will comb the dataset to predict the probability of whether an individual is a POI. Therefore, it is important that we build this algorithm with accurate underlying data. It is most likely that we will not need all 21 columns as “features” in the model, as some will have more predictive capabilities than others, and some may have no predictive capabilities at all. Finally, we may want to create new columns using outside data, or the provided dataset. We would do this if we expect the new columns to be more predictive of POIs than the information we currently have in its existing representation. Here are a few changes I made:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I threw away the “email_address” field as there isn’t meaningful information between the different values to help us identify a POI.&lt;/li&gt;
  &lt;li&gt;I threw away the ‘TRAVEL AGENCY IN THE PARK’ and ‘TOTAL’ rows as outliers.&lt;/li&gt;
  &lt;li&gt;I created several new “ratio” fields: the percentage of emails to or from a POI, the percentage of compensation that came in bonus, salary, and restricted stock.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;feature-selection-and-tuning&quot;&gt;Feature Selection and Tuning&lt;/h2&gt;

&lt;p&gt;The NaN values in the dataset are a blend of string and nan values. I converted all NaN values to a numpy nan (np.nan) value, and then converted the values to an outlier value (I used -0.42, which is close to (but not quite) the “Answer to the Ultimate Question of Life, the Universe, and Everything.” This allows the dataframe to be processed by classification algorithms that can’t take N/A values, such as Random Forest.&lt;/p&gt;

&lt;p&gt;I ran a Random Forest classification quickly to see what my preliminary values were—the answer was not very good. I had “mild” predictive capabilities for people who are NOT POIs, and 0% accuracy/recall/precision for predicting POIs. Part of the reason for this is the imbalance between the two classes (noted earlier, ~85% of the list are not POIs) and the model is trying to predict for the larger class. Fortunately, my colleague Tom Fawcett has written about this (see: &lt;a href=&quot;https://svds.com/learning-imbalanced-classes/&quot;&gt;Learning from Unbalanced Classes&lt;/a&gt;). I began building ROC curves and precision-recall curves to understand performance for different tuning parameters.&lt;/p&gt;

&lt;p&gt;To determine the right features and tuning parameters, I built a Pipeline using SelectKBest and GridSearchCV together. The pipeline applies the first step by choosing the best k features and transforms the input data to have only these features. After transformation, this is then fit with different estimators. GridSearchCV helps to tune the “number of features to be selected” and the hyperparameter of the estimator, by selecting the parameters that give the best score on validation data.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Sample code for testing SVMs:&lt;/em&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kbest = SelectKBest(f_classif)&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pipeline = Pipeline([('kbest', kbest), ('svm', SVC())])&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grid_search_svm = GridSearchCV(pipeline, {'kbest__k': [1,2,3,4], 'svm__C': [1, 2, 3, 4], 
                                      'svm__kernel': ['linear', 'poly', 'rbf', 'sigmoid'] })&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;grid_search_svm.fit(features.values, labels['poi'].values)&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;labels_pred_svm = grid_search_svm.predict(features_test)&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;print classification_report(labels_test, labels_pred_svm)&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;print confusion_matrix(labels_test, labels_pred_svm)&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;print grid_search_svm.best_params_&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;print plot_ROC_curve(grid_search_svm, features.values, labels['poi'].values)&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;print plot_PR_curve(grid_search_svm, features.values, labels['poi'].values)&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;selecting-an-algorithm-and-performance&quot;&gt;Selecting an Algorithm and Performance&lt;/h2&gt;

&lt;p&gt;I tested 3 different types of algorithms on the dataset with the pipeline method described above:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Decision Trees:&lt;/em&gt; A decision tree can be thought of as a flowchart that classifies samples based on feature performance. They are very useful in practice as it is easy to determine provenance—I can articulate how and why an algorithm makes a decision. This is helpful in industries like financial services, in which there are regulations against favoring or biasing against specific groups. The performance for POI prediction can be found below:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/decision-trees.png&quot; /&gt;
	&lt;figcaption&gt;Decision Tree performance.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Random Forests:&lt;/em&gt; A “random forest” is an ensemble model of different decision trees. Roughly speaking, random forests create several decision trees and then “blend” their results to create a newly predicted outcome. When it comes to accuracy, they are very useful in practice. At the same time, there is a greater challenge in understanding “how” predictions are made. Also, since multiple trees are created and blended, there is often a significant runtime associated with Random Forests, which needs to be accounted for in data products and analytic workflows. The performance for POI prediction can be found below:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/random-forest.png&quot; /&gt;
	&lt;figcaption&gt;Random Forest performance.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Gaussian NB:&lt;/em&gt; Naive Bayes classifiers treat different features as independent of one another. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features. It has been found useful in text retrieval and medical diagnoses. The performance for POI prediction can be found below:&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/Gaussian.png&quot; /&gt;
	&lt;figcaption&gt;Gaussian NB performance.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;These calculations are derived from each model’s confusion matrix, which is a table outlining how different predictions performed against a holdout set of data with known labels. If we treat the predictions as probabilities instead of hard labels (for example, an individual is “54% likely” to be a POI as opposed to only 0% or 100%), we can create an ROC chart as a metric for performance: an explanation of how to use ROC curves &lt;a href=&quot;http://gim.unmc.edu/dxtests/roc2.htm&quot;&gt;can be found here&lt;/a&gt;. Below is the ROC chart for Decision Trees:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/ROC-curve.png&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;The code I used split the data into 3 “folds”, each of which has different performance—the “mean ROC” is the final measure when deciding the performance of the algorithm. The greater the area indicates stronger predictiveness. There is no absolute threshold for determining good performance—one needs to consider the context surrounding the decision being made.
I found that Decision Trees performed the best, as measured by Precision, Recall, and F1 Score.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Precision is a measure of the number of identified True Positives divided by the total number of Positive test results (including False Positives). FPs are known as Type I Error.&lt;/li&gt;
  &lt;li&gt;Recall is the number of identified True Positives divided by the total number of actual Positive samples (including False Negatives). FNs are known as Type II Error.&lt;/li&gt;
  &lt;li&gt;A F1 Score is a measure of accuracy, calculated as the harmonic mean of Precision and Recall.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When looking at these scores, I was more focused on the ability to predict “1s” than the overall measures. The value 1 represents a POI. Since the data is skewed towards non-POIs (80%+ of the set), the overall numbers give the impression that the models are more highly predictive than what would be representative of their intended use. Similarly, since the goal of the activity is to identify POIs, we might be comfortable with a lower Precision measurement (increased FPs) if it provides a higher Recall (minimized FNs). Similar biases are present in medical testing.&lt;/p&gt;

&lt;p&gt;As an important note, all of my measurements regarding performance are calculated using a “holdout” set or a “fold” that is independent of the data upon which the model is constructed. This is referred to as “validation.” Validation is important because we are trying to generate models that have inference—some predictive capability—on a general population. We want to make sure that our models perform more effectively (much more effectively) than random chance.&lt;/p&gt;

&lt;p&gt;To do this, we use a set of data that the model &lt;em&gt;has never seen&lt;/em&gt; and may not be entirely representative of the distribution of data upon which the model was constructed. Articulating the model’s performance using these metrics helps ensure that we are not “overfitting” the data—models that are highly predictive of the datasets on which they were trained, but perform poorly in general practice. Here is &lt;a href=&quot;https://www.ibm.com/blogs/research/2015/08/preserving-validity-in-adaptive-data-analysis/&quot;&gt;a great article&lt;/a&gt; from IBM Research on preserving validity in analysis.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This was a fun project that helped hammer home some of the key concepts of data preparation and model selection. Much of my prior training in data science was done in R, and I wanted a guided introduction to scikit-learn, numpy, and pandas. It is amazing how transferable the approach and libraries are for the two languages—at the same time, there are funny anachronisms for how scikit-learn takes data from pandas dataframes vs. numpy arrays.&lt;/p&gt;</content><author><name></name></author><summary type="html">Figuring out what is different about Jeff Skilling.</summary></entry><entry><title type="html">Playing Around with Docker</title><link href="http://localhost:4000/playing-with-docker/" rel="alternate" type="text/html" title="Playing Around with Docker" /><published>2016-11-05T00:00:00-07:00</published><updated>2016-11-05T00:00:00-07:00</updated><id>http://localhost:4000/playing-with-docker</id><content type="html" xml:base="http://localhost:4000/playing-with-docker/">&lt;figure&gt;
	&lt;img src=&quot;/images/docker-jenkins.png&quot; /&gt;
	&lt;figcaption&gt;Refined delivery! (I love this image.)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I recently completed a great Docker tutorial offered by &lt;a href=&quot;http://prakhar.me/docker-curriculum/&quot;&gt;Prakhar Srivastav&lt;/a&gt;. In this exercise, we set up &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt;, log into the DockerHub, and pull down “images.” We also publish these images to the web, and create a dynamic website using EC2 Container Service using AWS and two docker images networked together.&lt;/p&gt;

&lt;h2 id=&quot;what-is-docker-what-are-containers&quot;&gt;What is Docker? What are “containers”?&lt;/h2&gt;

&lt;p&gt;The industry standard today is to use Virtual Machines (VMs) to run software applications. VMs run applications inside a guest Operating System, which runs on virtual hardware powered by the server’s host OS.&lt;/p&gt;

&lt;p&gt;VMs are great at providing full process isolation for applications: there are very few ways a problem in the host operating system can affect the software running in the guest operating system, and vice-versa. But this isolation comes at great cost — the computational overhead spent virtualizing hardware for a guest OS to use is substantial.&lt;/p&gt;

&lt;p&gt;Containers take a different approach: by leveraging the low-level mechanics of the host operating system, containers provide most of the isolation of virtual machines at a fraction of the computing power. Docker specifically interacts directly with the Linux kernel. Containerization has grown rapidly, especially with the growth of cloud computing and services. For example, &lt;a href=&quot;http://www.informationweek.com/cloud/infrastructure-as-a-service/google-docker-does-containers-right/d/d-id/1319146&quot;&gt;Google launches roughly 2 billion containers per week&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Why has the containerization movement risen with cloud computing? This lower overhead means that &lt;a href=&quot;https://blog.codeship.com/why-docker/&quot;&gt;cloud providers like Amazon and others getting a 26 to 1 performance improvement on the virtual machines that they sell per hour&lt;/a&gt;. It’s a huge enabler for their businesses because you’re suddenly able to do a lot more for the same price. Instead of needing to buy two virtual machines (for load balancing/availability) for every isolated application that you need to deploy, you could just cluster three larger VMs and deploy all of them to it, actual processor limits aside. It makes cloud computing cheaper for both AWS/Azure/GCP and the businesses consuming those services. When business makes cost-benefit decisions around cloud migrations, this resource efficiency creates a swing in favor of cloud providers.&lt;/p&gt;

&lt;p&gt;On the other side of the equation, containerization is well-loved by developers because it de-risks creating production level software:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Your development team is able to create complex requirements for a microservice within an easy-to-write Dockerfile.&lt;/li&gt;
  &lt;li&gt;Push the code up to your git repo.&lt;/li&gt;
  &lt;li&gt;Let the continuous integration (CI) server pull it down and build the EXACT environment that will be used in production to run the test suite without needing to configure the CI server at all.&lt;/li&gt;
  &lt;li&gt;Tear down the entire thing when it’s finished.&lt;/li&gt;
  &lt;li&gt;Deploy it out to a staging environment for testers or just notify the testers so that they can run a single command to configure and start the environment locally.&lt;/li&gt;
  &lt;li&gt;Confidently roll exactly what you had in development, testing, and staging into production without any concerns about machine configuration.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-an-image&quot;&gt;What is an image?&lt;/h2&gt;

&lt;p&gt;You can think of a docker “image” as a fully packaged container. It includes all of the executable code for an application, as well as defines the packages and dependencies required to run the application effectively. Often, images can be automatically stood up on a server with a few lines of code.&lt;/p&gt;

&lt;p&gt;Images can also be shared publicly in the &lt;a href=&quot;https://store.docker.com/search?category=application_framework&amp;amp;source=verified&quot;&gt;Docker Store&lt;/a&gt;. In the Docker store, you can find projects that other teams are willing to publish—helping you see pre-existing code or logic that may help you get your projects off the ground faster. In this sense, it has functionality similar to Github, a site in which teams manage version control of their codebase, but users can also publish and share their code publicly.&lt;/p&gt;

&lt;p&gt;Prakar’s exercises involves loading a single image of cat photos, and then a slightly more complex site of two images networked together, seen here:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/foodtrucks.png&quot; /&gt;
	&lt;figcaption&gt;The final view of Prakhar's project.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We also get experience with pushing containers to AWS’s EC2 Container Service. I would recommend closing down the instances once you get your IP address up and running / know the project is live — I found that hosting the project on 2 containers cost ~$50/month.&lt;/p&gt;

&lt;p&gt;In the above picture, the two images are a base image (the underlying map) and an elasticsearch image. This allows site owners to troubleshoot and edit different aspects of the site independently—for example, if aspects of the “search” experience were broken, users would still be able to utilize other parts of the site. This functionality also allows the site to scale only on the services that are finding increased use.&lt;/p&gt;

&lt;h2 id=&quot;how-does-containerization-enable-microservices&quot;&gt;How does containerization enable microservices?&lt;/h2&gt;

&lt;p&gt;I was curious about Docker as a representation of the &lt;a href=&quot;http://www.martinfowler.com/articles/microservices.html&quot;&gt;microservices movement&lt;/a&gt;. The microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. In the last few years, it has become the default style for building enterprise applications and is considered to be a key enabler of software platforms. It is a validation of Jeff Bezos’s mandate to have all teams communicate their work through externalizable APIs (reference: &lt;a href=&quot;https://plus.google.com/+RipRowan/posts/eVeouesvaVX&quot;&gt;Steve Yegge’s insightful and fun “rant”&lt;/a&gt;).&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/microservices.png&quot; /&gt;
	&lt;figcaption&gt;Differences between a &quot;monolithic&quot; and a &quot;microservices&quot; design.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In service-oriented architectures (SOA), integration relies heavily on middleware, in particular enterprise service bus (ESB). Microservices architecture may often make use of a message bus, but there is no logic in the messaging layer whatsoever—it is purely used as a transport for messages from one service to another. This differs dramatically from ESB, which contains substantial logic for message routing, schema validation, message translation, and business rules. As a result, microservices architectures are substantially less cumbersome than traditional SOA, and don’t require the same level of governance and canonical data modeling to define the interface between services. (Source found &lt;a href=&quot;https://medium.com/aws-activate-startup-blog/using-containers-to-build-a-microservices-architecture-6e1b8bacb7d1#.yul4pgcng&quot;&gt;here&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;Docker provides an isolation between containers running on the same host that makes deploying microservice code developed using different languages and frameworks very easy. Using Docker, we could create a DockerFile describing all the language, framework, and library dependencies for that service. With microservices, development is rapid and services evolve alongside the needs of the business.&lt;/p&gt;

&lt;h2 id=&quot;what-next&quot;&gt;What next?&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Understanding Benefits to Web Design, Challenges with Pipelines/Clusters:&lt;/em&gt; In having conversation with data engineers at SVDS, we disucced the complications associated with Docker networking - which is a problem for distributed applications (Docker is trying to manage this with &lt;a href=&quot;https://technologyconversations.com/2015/11/04/docker-clustering-tools-compared-kubernetes-vs-docker-swarm/&quot;&gt;Docker Swarm&lt;/a&gt;). Their opinion was that it works great for web apps. It doesn’t really change “per environment.” These “scale out” in easily parallelizable ways. This gets a little bit tricker with things that have to communicate with each other—like Cassandra.&lt;/p&gt;

&lt;p&gt;They questioned how well Docker could solve the problem of Spark executions. The class of applications in which things can be modeled as a web app are situations in which containers can help a lot. In the class of stuff that we do—in building platforms — containers and networking complicate that, and the benefits of containers don’t really materialize when execution can’t take place there.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Network Bounds and Product Management:&lt;/em&gt; At SVDS, we might use Docker between each other for development. Then it would play the same role as a &lt;a href=&quot;https://www.vagrantup.com/&quot;&gt;Vagrant&lt;/a&gt;, for example. It should be said that one have to be strong at deploying microservices well (&lt;a href=&quot;https://12factor.net/&quot;&gt;general design principles&lt;/a&gt;), before even thinking about how to isolate and decouple the functionality. In that sense, each team will need to have good explicit or implicit product management.&lt;/p&gt;

&lt;p&gt;From a performance standpoint, if 40 services are needed to load networking has to be incredibly robust. There are a lot of pings back and forth.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Additional link: &lt;a href=&quot;https://gist.github.com/hellerbarde/2843375&quot;&gt;Pretty cool chart&lt;/a&gt; I found on latency from Peter Norvig and Jeff Dean.&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Taming the Whale.</summary></entry><entry><title type="html">Database Primer: Redis</title><link href="http://localhost:4000/database-primer-redis/" rel="alternate" type="text/html" title="Database Primer: Redis" /><published>2016-10-01T00:00:00-07:00</published><updated>2016-10-01T00:00:00-07:00</updated><id>http://localhost:4000/database-primer-redis</id><content type="html" xml:base="http://localhost:4000/database-primer-redis/">&lt;p&gt;A key tension when choosing an infrastructure concerns how to manage product performance with issues of extensibility (ie, confidence in future usability of a language or tool) and ongoing maintenance. Part of this tension is that infrastructure management is a dynamic effort. For example, distributed (and/or NoSQL) systems, but they &lt;a href=&quot;http://highscalability.com/blog/2016/1/11/a-beginners-guide-to-scaling-to-11-million-users-on-amazons.html&quot;&gt;only become pertinent as a company’s user base moves into the millions (or tens of millions)&lt;/a&gt;. This is the point at which the organization might run into database issues around contention with the write master, which basically means you’re sending too much write traffic to one server.&lt;/p&gt;

&lt;p&gt;Part of the tension is rooted in experience: large companies, some of whom may have little experience with managing a digital-scale infrastructure, are being asked to adopt these evolving technologies that require a very specific skillset. On top of this, a common adage is that &lt;a href=&quot;http://firstround.com/review/the-three-infrastructure-mistakes-your-company-must-not-make/&quot;&gt;the best operators won’t use a component until they know how it breaks&lt;/a&gt;. So, how might one generate this experience?&lt;/p&gt;

&lt;p&gt;To start, I’ve pulled together a few primers on more commonly used NoSQL databases to give a sense of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When they are used (use cases), and&lt;/li&gt;
  &lt;li&gt;Common considerations / limitations / challenges&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ve covered &lt;a href=&quot;http://bradaallen.com/database-primer-mongodb/&quot;&gt;MongoDB&lt;/a&gt; and &lt;a href=&quot;http://bradaallen.com/database-primer-hbase/&quot;&gt;HBase&lt;/a&gt; in prior posts—for this one, let’s look at Redis.&lt;/p&gt;

&lt;p&gt;##Redis Background&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://redis.io/&quot;&gt;Redis&lt;/a&gt; is an in-memory data structure store frequently used as a database, cache, or message broker. It is popular with developers because of its versatile, optimized data structures – sets, sorted sets, hashes, lists, strings, bit arrays – which deliver efficient in-database operations such as set comparisons, list pull-push operations, and range queries.&lt;/p&gt;

&lt;p&gt;Redis is also used for sessionizing data. Every event collected from an application or website belongs to a session that a user starts against Redis. Documents with tens or thousands of events create a stream of data that requires some amount of unraveling. When there are hundreds and thousands of users, event streams related to many users are interleaved. Updating each document with many small updates can be accomplished with disk-based MongoDB, but the expense of the operation is high and Redis’ hash data structure can make short work of the problem.&lt;/p&gt;

&lt;p&gt;Hashes can be used to store event data by session. Keeping track of sessions that need to be timed out is also nontrivial when there are thousands of sessions, but Redis has built-in key expiration and timeout setting functionality that can be used to end sessions. Keyspace notifications allow users to subscribe to expired events and trigger an offload to MongoDB.&lt;/p&gt;

&lt;p&gt;##Common Considerations with Redis&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Redis doesn’t provide sharding.&lt;/em&gt; You should probably assume that you’ll grow beyond the capacity of a single Redis server (slaves are for redundancy, not for scaling, though you can offload some read-only operations to slaves if you have some way to manage the data consistency — for example the ZSET of key/timestamp values describe for expiry can also be used for some offline bulk processing operations; also the pub/sub features can be used for the master to provide hints regarding the quiescence of selected keys/data.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Consider writing your own abstraction layer to provide sharding.&lt;/em&gt; Basically imagine that you have implemented a consistent hashing method … you run every synthesized key through that before you use it.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Choose consistent ways to name and prefix your keys.&lt;/em&gt; Manage your namespace. Create a “registry” of key prefixes which maps each to your internal (perhaps wiki) documents for those application which “own” them.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Design, implement and test the mechanisms for garbage collection and/or data migration&lt;/em&gt; for every class of data you put into your Redis infrastructure.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Design, implement and test a sharding (consistent hashing) library before you’ve invested much&lt;/em&gt; into your application deployment and ensure that you keep a registry of “shards” replicated on each server.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Isolate all your K/V store and related operations into a your own library/API or service&lt;/em&gt; And absolutely enforce the use of that API/service with an unrelenting and mercilessly iron hand.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Don’t run out of RAM.&lt;/em&gt; You need enough RAM to hold your entire set of data. Don’t expect to be able to query data like in a database.  The paradigm of searching data is completely different.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hopefully this record helps clarify some situations in which Redis might be useful, and some associated maintenance considerations. Many thanks to these articles as the primary references when putting this together:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/What-are-5-mistakes-to-avoid-when-using-Redis&quot;&gt;What are 5 mistakes to avoid when using Redis? (Quora)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.infoworld.com/article/3008052/nosql/mongodb-and-redis-pair-volume-with-velocity.html&quot;&gt;MongoDB and Redis pair volume with velocity (InfoWorld)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Figuring out what Redis is all about.</summary></entry><entry><title type="html">Algorithms 101: Walking through the greatest hits.</title><link href="http://localhost:4000/algorithms-101/" rel="alternate" type="text/html" title="Algorithms 101: Walking through the greatest hits." /><published>2016-09-05T00:00:00-07:00</published><updated>2016-09-05T00:00:00-07:00</updated><id>http://localhost:4000/algorithms-101</id><content type="html" xml:base="http://localhost:4000/algorithms-101/">&lt;p&gt;I recently completed two courses on algorithms at Coursera: Roughgarden’s &lt;a href=&quot;https://www.coursera.org/learn/algorithm-design-analysis/home/welcome&quot;&gt;“Algorithms: Design &amp;amp; Analysis”&lt;/a&gt; and Sedgewick’s &lt;a href=&quot;https://www.coursera.org/learn/introduction-to-algorithms/home/welcome&quot;&gt;“Introduction to Algorithms.”&lt;/a&gt; Both covered very similar material—a “greatest hits” of classic algorithms (quicksort, graphing—e.g., Djikstra’s shortest paths) and a few of the underlying data types that facilitate efficient runtime (e.g., heaps, search trees, hashing).&lt;/p&gt;

&lt;p&gt;Although they were very similar in content, the two were very different in presentation. Sedgewick’s course assignments are java-based, with very strict APIs on memory management, code quality, etc. The video lectures then walked through the code required to implement the algorithms—with visualizations to give an intuitive sense of runtime. Roughgarden’s course is much more theoretical—for example, explaining how to calculate topological ordering or strongly connected components through depth- or breadth-first searches. It also explains the differences between different “O” notations, and asks students to think about work / computation and the associated number of operations required for a given algorithm.&lt;/p&gt;

&lt;p&gt;##Was it worth it?&lt;/p&gt;

&lt;p&gt;With any type of online course, the hope is that the material will be targeted learning that has direct daily applications. My friends who have CS degrees hear “introductory algorithms course” and either think of it as a rite of passage a long, long time ago or an intellectual exercise. The expectation is that this course, in practice, would have little utility.&lt;/p&gt;

&lt;p&gt;I didn’t find this to be the case. Having now studied different NoSQL database types and taken a few certifications on data science, I found the courses to be incredibly useful—perhaps because I can now refer back to times that I “brute forced” a fuzzy join (using double for loops) that would have been ~1000x faster with a hash. A few key lessons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;When runtime can be managed.&lt;/em&gt; Knuth, Sedgewick, Tarjan—taking this course, it feels like the 1960s and 1970s were full of very smart guys who were very good at discrete mathematics. As a result, we are able to put all sorts of use cases into production that wouldn’t be possible without clever workarounds. It’s really useful to have an intuition for the difference between quadratic, logarithmic, linearithmic, and constant time in the code.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;The relationship between data structures and algorithms.&lt;/em&gt; The projects I’ve worked with, to date, have been files with the data in a table or an array. I did not have exposure to heaps or search trees. It’s amazing the performance improvements one can create in basic searching by changing the underlying structure of the data. One quote I enjoyed in Sedgewick’s class was from Linus Torvalds, regarding the difference between a bad programmer and a good one: “Bad programmers worry about the code, good programmers worry about data structures, and their relationships.” I have a much better intuition for why this is true, and how to think about product performance at scale.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;A new way of thinking about NoSQL databases.&lt;/em&gt; When I started at SVDS, I took some time to study different NoSQL database types. I ended up with a catalogue of different structures and associated use cases (e.g., When do you use Redis? When do you use Cassandra?). However, I did not have an intuition for why read-write performance might be different between these database structures and the challenges / tension that this may have with being able to query the data. After this course, it is much more clear how those use cases emerge and how architects make their infrastructure decisions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;##What does this lead to next?&lt;/p&gt;

&lt;p&gt;These courses were an inspiration for me to push further on application development and to think about production-level code. I’m putting together a project to set up a weather sensor at my home in San Francisco and create some visualizations / analytics against a &lt;a href=&quot;https://www.pubnub.com/developers/realtime-data-streams/state-capital-weather/&quot;&gt;streaming “weather in state capitols” dataset&lt;/a&gt; and the &lt;a href=&quot;https://www.wunderground.com/weather/api/&quot;&gt;WeatherUnderground API&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;A few additional topics I’d like to pull the thread on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Distributed Systems.&lt;/em&gt; How do algorithms and data management change when data is stored in different clusters?&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Model Scoring and Management.&lt;/em&gt; Java seems like a default language for production-level code because it is widely used and extensible. However, many models are generated in statistical packages—e.g., in R, scikit-learn in Python, or SAS. How are these models put into production effectively, and how is runtime managed for models that are complex by nature (e.g., the blended models in a Random Forest)?&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;All non-database related factors that have an effect on user runtime.&lt;/em&gt; For example, what is the effect of networking on runtime? I currently don’t have an intuition for all of the different things that affect the user experience and the different tools a product designer might have to manage an excellent experience—I am excited to learn more, though.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Gaining intuition with theory and Java.</summary></entry><entry><title type="html">Database Primer: HBase</title><link href="http://localhost:4000/database-primer-hbase/" rel="alternate" type="text/html" title="Database Primer: HBase" /><published>2016-08-24T00:00:00-07:00</published><updated>2016-08-24T00:00:00-07:00</updated><id>http://localhost:4000/database-primer-hbase</id><content type="html" xml:base="http://localhost:4000/database-primer-hbase/">&lt;p&gt;A key tension when choosing an infrastructure concerns how to manage product performance with issues of extensibility (ie, confidence in future usability of a language or tool) and ongoing maintenance. Part of this tension is that infrastructure management is a dynamic effort. For example, distributed (and/or NoSQL) systems, but they &lt;a href=&quot;http://highscalability.com/blog/2016/1/11/a-beginners-guide-to-scaling-to-11-million-users-on-amazons.html&quot;&gt;only become pertinent as a company’s user base moves into the millions (or tens of millions)&lt;/a&gt;. This is the point at which the organization might run into database issues around contention with the write master, which basically means you’re sending too much write traffic to one server.&lt;/p&gt;

&lt;p&gt;Part of the tension is rooted in experience: large companies, some of whom may have little experience with managing a digital-scale infrastructure, are being asked to adopt these evolving technologies that require a very specific skillset. On top of this, a common adage is that &lt;a href=&quot;http://firstround.com/review/the-three-infrastructure-mistakes-your-company-must-not-make/&quot;&gt;the best operators won’t use a component until they know how it breaks&lt;/a&gt;. So, how might one generate this experience?&lt;/p&gt;

&lt;p&gt;To start, I’ve pulled together a few primers on more commonly used NoSQL databases to give a sense of:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;When they are used (use cases), and&lt;/li&gt;
  &lt;li&gt;Common considerations / limitations / challenges&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ve covered MongoDB &lt;a href=&quot;http://bradaallen.com/database-primer-mongodb/&quot;&gt;in a prior post&lt;/a&gt;—for this one, let’s look at HBase.&lt;/p&gt;

&lt;p&gt;##HBase Background&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://hbase.apache.org/&quot;&gt;HBase&lt;/a&gt; is an open source, non-relational, distributed database modeled after Google’s BigTable and is written in Java. It is developed as part of Apache Software Foundation’s Apache Hadoop project and runs on top of HDFS (Hadoop Distributed File System), providing BigTable-like capabilities for Hadoop.&lt;/p&gt;

&lt;p&gt;HBase’s use has grown since being tapped to support &lt;a href=&quot;http://highscalability.com/blog/2010/11/16/facebooks-new-real-time-messaging-system-hbase-to-store-135.html&quot;&gt;Facebook’s Messenger application in 2010&lt;/a&gt;. Some characteristics of HBase:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Has a simpler consistency model than Cassandra.&lt;/li&gt;
  &lt;li&gt;Very good scalability and performance for their data patterns.&lt;/li&gt;
  &lt;li&gt;Most feature rich for their requirements: auto load balancing and failover, compression support, multiple shards per server, etc.&lt;/li&gt;
  &lt;li&gt;HDFS, the filesystem used by HBase, supports replication, end-to-end checksums, and automatic rebalancing.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HBase depends on all nodes in the cluster having closely synchronized clocks and referring to each other consistently. Using NTP and DNS ensures that you won’t run into odd behaviors when one node A thinks that the time is tomorrow and node B thinks it’s yesterday.&lt;/p&gt;

&lt;p&gt;##When should you look at HBase?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Your application has a variable schema where each row is slightly different&lt;/li&gt;
  &lt;li&gt;You can’t add columns fast enough and most of them are NULL in each row&lt;/li&gt;
  &lt;li&gt;You find that your data is stored in collections, for example some metadata, message data or binary data that is all keyed on the same value&lt;/li&gt;
  &lt;li&gt;You need key based access to data when storing or retrieving&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Common Considerations with HBase:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;HBase doesn’t talk SQL, have an optimizer, or support cross record transactions or joins.&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Be careful when running mixed workloads on an HBase cluster.&lt;/em&gt; When you have SLAs on HBase access independent of any MapReduce jobs (for example, a transformation in Pig and serving data from HBase) run them on separate clusters.&lt;/li&gt;
  &lt;li&gt;HBase is CPU and Memory intensive with sporadic large sequential I/O access while MapReduce jobs are primarily I/O bound with fixed memory and sporadic CPU.* Combined these can lead to unpredictable latencies for HBase and CPU contention between the two. A shared cluster also requires fewer task slots per node to accommodate for HBase CPU requirements (generally half the slots on each node that you would allocate without HBase).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Keep an eye on memory swap.&lt;/em&gt; If HBase starts to swap there is a good chance it will miss a heartbeat and get dropped from the cluster. On a busy cluster this may overload another region, causing it to swap and a cascade of failures.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Compactions that disrupt operations.&lt;/em&gt; HBase like most other NoSQL databases, stores disk writes in a sequential fashion in a write-ahead log. However, these logs at some point have to be merged and reconciled leading to heavy disk I/O. These I/O storms called compactions, significantly slow down the application and lead to spikes in response time, affecting system reliability and performance.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Very slow crash recovery.&lt;/em&gt; When a node fails in an HBase cluster, it could easily take between 30 minutes and 4 hours to recover it. This is because HBase relies on a RegionServer architecture that has several moving parts that need to be reestablished and realigned for the database to come back to normal operations.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Unreliable splitting.&lt;/em&gt; When one region has too much data, it needs to be split into two, leading to system unreliability. Users have tried to avoid the issue by pre-splitting the tables that requires a lot of manual work and guesstimating the size of the regions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hopefully this record helps clarify some situations in which HBase might be useful, and some associated maintenance considerations. Many thanks to these articles as the primary references when putting this together:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/vanuganti/hbase-hadoop-hbaseoperationspractices&quot;&gt;HBase Operations and Best Practices (LinkedIn Blog)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.cloudera.com/blog/2011/04/hbase-dos-and-donts/&quot;&gt;Apache HBase Do’s and Don’ts (Cloudera)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://highscalability.com/blog/2010/11/16/facebooks-new-real-time-messaging-system-hbase-to-store-135.html&quot;&gt;Facebook’s New Real-Time Messaging System: HBase To Store 135+ Billion Messages A Month (High Scalability)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.toadworld.com/platforms/nosql/b/weblog/archive/2013/10/14/avoiding-hbase-reliability-problems&quot;&gt;Avoiding HBase Reliability Problems (Toad World)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Figuring out what HBase is all about.</summary></entry></feed>