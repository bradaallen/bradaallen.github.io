<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-07T12:55:10-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Perpetually Learning</title><subtitle>Brad Allen's personal blog. I love building things (and learning how to build new things). Preferably those that scale. Keywords: Product Management, Data Science, Data Engineering, Python, Stanford, Stanford GSB, Penn State, Pennsylvania, TFA, Teach For America, Bain, Embrace, Aclima, Peloton Technology, Mechatronics, C+
</subtitle><entry><title type="html">Moving hosting from AWS to Github.</title><link href="http://localhost:4000/moving-to-github/" rel="alternate" type="text/html" title="Moving hosting from AWS to Github." /><published>2020-02-14T00:00:00-08:00</published><updated>2020-02-14T00:00:00-08:00</updated><id>http://localhost:4000/moving-to-github</id><content type="html" xml:base="http://localhost:4000/moving-to-github/">&lt;p&gt;&lt;em&gt;This article was written in 2017 during my time at SVDS. The article was first published on the &lt;a href=&quot;https://www.svds.com/customer-journey-set-success/&quot;&gt;SVDS Blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In the &lt;a href=&quot;https://bradaallen.github.io/customer-journey-success-part-1/&quot;&gt;introductory post&lt;/a&gt;, we walked through some examples of how SVDS has seen data capabilities determine the success of customer journey initiatives for our clients. In this post, we offer guidance on the data-related initiatives that you can start today to begin fostering closer ties with your customers—regardless of where you currently are in your specific state of development.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;GH Pages:1. Use WSL, don’t mess with the devkits: https://kiazhi.github.io/blog/Working-with-Jekyll-and-Ruby-on-Windows-for-GitHub-Pages/#getting-started-with-jekyll-on-windows-10-using-windows-subsystem-for-linux&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Use rbenv, don’t mess with bare installations: https://brainwipe.github.io/jekyll/hyde/ubuntu/2017/05/16/pulling-jekyll-teeth/&lt;/li&gt;
  &lt;li&gt;Make sure you are using the gh-pages dependencies for Ruby and Jekyll: https://pages.github.com/versions/* if you had other versions pre-installed, make sure you are loading from rbenv (eg, the shims in ruby and jekyll)&lt;/li&gt;
  &lt;li&gt;Pick your theme; I like minimal mistakes: https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/&lt;/li&gt;
  &lt;li&gt;Follow this site for then creating your GH repo, testing locally, etc: https://help.github.com/en/github/working-with-github-pages/creating-a-github-pages-site-with-jekyll&lt;/li&gt;
  &lt;li&gt;Set up a custom domain, if desired: https://help.github.com/en/github/working-with-github-pages/configuring-a-custom-domain-for-your-github-pages-site
Don’t forget the ~/.bashrc and . in jekyll new! rbenv init -, etc.
https://bryancshepherd.com/data-science/setting-bluehost-dns-github-jekyll-blog/  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;https://help.github.com/en/github/working-with-github-pages/configuring-a-custom-domain-for-your-github-pages-site 
https://help.github.com/en/github/working-with-github-pages/managing-a-custom-domain-for-your-github-pages-site
https://medium.com/@kimcodes/setting-up-a-web-page-with-github-pages-f77d45573ab2&lt;/p&gt;</content><author><name></name></author><summary type="html">Quick notes on setting up a gh-pages blog using Windows 10.</summary></entry><entry><title type="html">Job Skills &amp;amp; Automation: The importance of demand.</title><link href="http://localhost:4000/skills-and-automation-demand/" rel="alternate" type="text/html" title="Job Skills &amp; Automation: The importance of demand." /><published>2020-02-01T00:00:00-08:00</published><updated>2020-02-01T00:00:00-08:00</updated><id>http://localhost:4000/skills-and-automation-demand</id><content type="html" xml:base="http://localhost:4000/skills-and-automation-demand/">&lt;p&gt;&lt;em&gt;Every few years, I review literature on productivity and automation. As a data scientist working in large enterprises, I have a front row seat to an emerging skillset that fits into our digital economy. As a former educator, I have a specific bias towards skill and capability development. 
These reviews are an attempt to understand some of the prevailing economic forces today, and identify the best next steps forward for creating opportunity for all. This article highlights the role of product or service demand in job loss and job reskilling.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;are-robots-taking-our-jobs-no---but-the-nature-of-work-is-changing&quot;&gt;Are robots taking our jobs? No - but the nature of work is changing.&lt;/h3&gt;

&lt;p&gt;There have been many reports of widespread job loss due to artificial intelligence, which are continually being refined and hedged back. Pew Research reported in 2018 that &lt;a href=&quot;https://www.pewresearch.org/global/2018/09/13/in-advanced-and-emerging-economies-alike-worries-about-job-automation/&quot;&gt;65% of Americans believe most jobs will be lost in the next 50 years&lt;/a&gt;. An Oxford University study estimated that &lt;a href=&quot;https://www.oxfordmartin.ox.ac.uk/downloads/academic/The_Future_of_Employment.pdf&quot;&gt;“47 percent of U.S. workers have a high probability of seeing their jobs automated over the next 20 years.”&lt;/a&gt;  It was even a primary component of &lt;a href=&quot;https://www.yang2020.com/what-is-freedom-dividend-faq/&quot;&gt;Andrew Yang’s presidential platform&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Experience and research would suggest that these projections are sensational, and divorced from the practical drivers associated with the change. For example, it’s important to have an accurate baseline for natural technological progress. In the decade between 2006 and 2016, over &lt;a href=&quot;https://www.americanactionforum.org/insight/understanding-job-loss-predictions-from-artificial-intelligence&quot;&gt;51 million jobs were destroyed, while 179 million jobs were created&lt;/a&gt;. Should we highlight the -51 million number? Or the net +130 million number?&lt;/p&gt;

&lt;p&gt;Additionally, automation is primarily a task-based endeavor - it does not create wholesale job loss, just parts of jobs. The associated process redesign that is required for people ot adopt new skills and tools acts both as a natural governor on the rate of change and suggests that the value created must be far in excess of the pain requirred to have folks change their jobs.&lt;/p&gt;

&lt;p&gt;A third reason to question sensational headlines: automation changes the cost of production, which may have an effect on demand. It’s important to understand product or market elasticity before making assumptions about turnover or loss. What are consumer habits when a product or service is both cheaper and faster?&lt;/p&gt;

&lt;p&gt;This all being said, it is important to have a perspective on the impact automation will have on your business. The opportunity afforded by machine learning and automation is material. Organizations need to prepare their workers for lifelong learning and have a vision for how to develop their workforce to manage the change. The following section walks through recent research on this topic and some of the policies, programs, and initiatives being established to support the change.&lt;/p&gt;

&lt;h3 id=&quot;understanding-demand-is-a-critical-lever-in-understanding-the-rate-of-job-turnover&quot;&gt;Understanding demand is a critical lever in understanding the rate of job turnover&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.nber.org/papers/w24235.pdf&quot;&gt;Artificial intelligence (AI) technologies will automate many tasks&lt;/a&gt;, but the effect on employment is
not obvious. For example, in manufacturing, technology has sharply reduced jobs in recent decades. But before that, for over a century, employment grew, even in industries experiencing rapid technological change. What changed? Demand was highly elastic at first and then became inelastic.&lt;/p&gt;

&lt;p&gt;The effect of artificial intelligence on jobs will similarly depend critically on the nature of demand. New technologies do not just replace labor with machines, but, in a competitive market, automation will reduce prices. In addition, technology may improve product quality, customization or speed of delivery. All of these things can increase demand. If demand increases sufficiently, employment will grow even though the labor required per unit of output declines.&lt;/p&gt;

&lt;p&gt;This relationship can be seen in &lt;a href=&quot;https://www.brookings.edu/wp-content/uploads/2020/01/Bessen-et-al_Full-report.pdf&quot;&gt;the “Inverted U” diagram shown below&lt;/a&gt;. Although labor productivity in cotton textiles increased nearly 30-fold during the 19th century, consumption of cotton cloth increased 100-fold.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/textile_workers_over_time.PNG&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;It is an empirical matter to determine whether industries have pent up demand — especially if they have seen relatively little automation to date — or an inelastic demand response. A more recent example involves &lt;a href=&quot;https://www.aei.org/economics/what-atms-bank-tellers-rise-robots-and-jobs/&quot;&gt;the interaction between bank tellers and the introduction of the ATM&lt;/a&gt;.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/tellers_atms_over_time.PNG&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;With the introduction of the ATM, the average bank branch in an urban area was reduced from 21 tellers to 13 - making it cheaper to manage a branch. When it became cheaper to do so, demand for branch offices (and more bank tellers, but with a different mix of responsibilities) increased as well. As can be seen above, this was enough to offset the labor-saving losses of jobs that would have otherwise occurred.&lt;/p&gt;

&lt;h3 id=&quot;newly-created-or-modified-jobs-will-be-different-from-those-eliminated-by-automation&quot;&gt;Newly created (or modified) jobs will be different from those eliminated by automation&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.brookings.edu/wp-content/uploads/2020/01/Bessen-et-al_Full-report.pdf&quot;&gt;The challenge of automation in the near future&lt;/a&gt; will likely not be mass unemployment, but, instead, a greater level of worker transitions. These transitions may involve temporary unemployment spells and a loss of income. Automation thus places a burden on workers even if, in the end, they do not permanently lose employment. Moreover, inefficient transitions can slow the productivity-enhancing promise of new technology.&lt;/p&gt;

&lt;p&gt;One can estimate the rate of job change and loss by &lt;a href=&quot;https://www.hoover.org/research/how-will-machine-learning-transform-labor-market&quot;&gt;focusing on what ML can do with respect to the tasks currently done by workers&lt;/a&gt;. To do this, economist Erik Brynjolfsson built a rubric to identify which tasks (and jobs) in the economy have the greatest “suitability for machine learning” (SML).&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/degree_of_automation.PNG&quot; /&gt;
	&lt;figcaption&gt;In this chart, 1 is “least automatable” and 5 is “most automatable.”&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The fact that most occupations fall between these extremes underscores the likelihood that machine learning will drive re-organization and re-engineering of how tasks are bundled and assigned into occupations. Indeed, Brynjolfsson et al. (2019), highlight that re-organization of work, not automation or substitution, is the labor demand force with the greatest economic potential for ML. These changes are heterogenous by industry and &lt;a href=&quot;https://www.nber.org/papers/w24001.pdf&quot;&gt;may take decades to have the full effect migrate through the economy&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Even with the differences in the rate and nature of change associated with automation, it is important to acknowledge that a substantial portion of the workforce is now, and will continue to be, affected each year. Workers at these automating firms experience non-negligible income losses - in &lt;a href=&quot;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3328877&quot;&gt;a study of the Netherlands market by James Bessen&lt;/a&gt;, this was estimated to be about 11% of a year’s pay on average. “There is a serious social challenge,” Bessen says. “Even in a place like the Netherlands that’s supposed to have a great social safety net—it’s not working.”&lt;/p&gt;</content><author><name></name></author><summary type="html">Folks will need new skills more than they'll need new jobs.</summary></entry><entry><title type="html">A Diverging Economy: ‘Superstar firms’ and wage dispersion.</title><link href="http://localhost:4000/digital-economy-superstar-firms/" rel="alternate" type="text/html" title="A Diverging Economy: 'Superstar firms' and wage dispersion." /><published>2020-01-15T00:00:00-08:00</published><updated>2020-01-15T00:00:00-08:00</updated><id>http://localhost:4000/digital-economy-superstar-firms</id><content type="html" xml:base="http://localhost:4000/digital-economy-superstar-firms/">&lt;p&gt;&lt;em&gt;When I wrote my first articles on the digital economy in 2016, many economists were writing about a “productivity puzzle.” This article explores the role of ICT in those changes. The introduction can be found &lt;a href=&quot;https://www.newyorker.com/news/john-cassidy/the-great-productivity-puzzle&quot;&gt;here&lt;/a&gt;. Additional articles on the relationship between job skills &amp;amp; automation (on demand, retraining, and policy goals) can be found in the links.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;industry-concentration-and-the-rise-of-superstar-firms&quot;&gt;Industry concentration and the rise of “superstar firms”&lt;/h3&gt;

&lt;p&gt;Over the last ~30 years, most industries in the US economy have seen a rising concentration of sales to the largest firms, as well as large geographic expansion of the largest firms.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/sf_fed_research.PNG&quot; /&gt;
	&lt;figcaption&gt;Retail trade share of revenue by Top 30 firms over time; Number of establishments by firm, disaggregated by # of FTEs.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;a href=&quot;https://www.frbsf.org/economic-research/publications/economic-letter/2019/november/is-rising-concentration-hampering-productivity-growth/&quot;&gt;A recent paper by the SF Fed&lt;/a&gt; hypothesizes that this rise in industry concentration is due to effective application of IT - that is, that innovation in ERP, SCM, CRM, FPA, etc. software has allowed the largest firms to expand geographically while retaining their ability to execute effectively.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.richmondfed.org/-/media/richmondfedorg/publications/research/working_papers/2018/pdf/wp18-15.pdf&quot;&gt;A Richmond Fed analysis&lt;/a&gt; on the same topic explores how this geographic expansion (and resulting concentration) has disproportionately favored the largest company in an industry. Over the last 30 years, the largest company has expanded much more rapidly than the #2 and #3 players - often, in local markets, the national leader is competing with smaller regional firms, not their national peers.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/richmond_fed_research.PNG&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;The team was surprised to find that, in their analysis, large enterprises do not enter and dominate the local market but instead lower its concentration, either by competing with the previous local monopolist or by simply adding one more establishment that grabs a proportional market share from other local establishments.&lt;/p&gt;

&lt;p&gt;This IT expansion, national concentration, and local competition &lt;a href=&quot;https://scholar.harvard.edu/aghion/publications/theory-falling-growth-and-rising-rents&quot;&gt;may be linked to the current low pace of growth through two steps&lt;/a&gt; (Aghion et al. (2019)): first, the IT expansion increased local competition, and second, increased competition eventually deterred innovation. Those that have innovated effectively have done so in a way that have challenged their competitions’ profit, making it even riskier to invest in innovative activities. These conclusions have prompted the &lt;a href=&quot;https://www.focus-economics.com/blog/why-is-productivity-growth-so-low-23-economic-experts-weigh-in&quot;&gt;economist Tim Taylor to ask&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Are these dynamics a result of weaker firms’ inability to make productivity-related improvements (perhaps because they lack organizational or human capital needed to do so)?&lt;/li&gt;
  &lt;li&gt;Or do these firms lack incentives to make such investments (perhaps because it is hard for firms to be confident of earning a profit on such investments)?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From this perspective, one might conclude that the decline in productivity is a result of differentiation in firms’ business strategy. However, this has always been the case - why does the modern economy have a resultant drag in productivity relative to times prior?&lt;/p&gt;

&lt;h4 id=&quot;the-role-of-banking-in-the-productivity-puzzle&quot;&gt;The role of banking in the productivity puzzle&lt;/h4&gt;

&lt;p&gt;Could weak productivity be a result of weak businesses staying alive longer? Current research indicates this may be true. In 2018, the Bureau for International Settlements did an &lt;a href=&quot;https://www.bis.org/publ/qtrpdf/r_qt1809g.pdf&quot;&gt;analysis on “zombie companies”&lt;/a&gt; - those that create enough cash to pay the interest of their debt, but not the principal. Analyzing across 14 advanced economies, the share of “zombie companies” rose, on average, from around 2% in the late 1980s to ~12% in 2016.&lt;/p&gt;

&lt;p&gt;How can corporate zombies survive for longer than in the past? They seem to face less pressure to reduce debt and cut back activity. The literature has identified weak banks as a potential key cause (&lt;a href=&quot;https://www.jstor.org/stable/29730158?seq=1&quot;&gt;Caballero et al (2008)&lt;/a&gt;). When their balance sheets are impaired, banks have incentives to roll over loans to non-viable firms rather than writing them off. Another potential, more general factor is the downward trend in interest rates. Mechanically, lower rates should reduce our measure of zombie firms as they improve ICRs by reducing interest expenses, all else equal. However, low rates can also reduce the pressure on creditors to clean up their balance sheets and encourage them to “evergreen” loans to zombies (&lt;a href=&quot;https://www.bis.org/publ/work628.pdf&quot;&gt;Borio and Hofmann (2017)&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This has also been explored in conferences by the OECD (eg, &lt;a href=&quot;http://www.oecd.org/global-forum-productivity/events/summary-record-bis-imf-oecd-conference.pdf&quot;&gt;“Weak Productivity: The Role of Financial Factors and Policies”&lt;/a&gt;). Here, it was acknowledged that lower rates could have played an unintended role in slowing the cleansing effect of the crisis - for example, by lowering the productivity thresholds afforded by the low interest rate environment. At the same time, conference leaders indicated that early normalization of monetary policy to boost productivity would not pass a cost benefit test—the costs are well-known (lower output, higher unemployment, risks of de-anchoring inflation expectations) and would be large, while the benefits (enhanced resource allocation) are highly uncertain and may be quite small.&lt;/p&gt;

&lt;p&gt;It is my belief that, through advances in data and analytics, these dynamics may continue (and might accelerate) in the coming years. New, more productive ways of operating, are being developed - by the already successful superstar firms. (Edit: This is the topic of my friend Alex’s &lt;a href=&quot;https://www.amazon.com/Always-Day-One-Titans-Forever-ebook/&quot;&gt;new book&lt;/a&gt;). In addition, any recessionary activity will likely be exacerbated by increased firm exits, particularly as interest rates are already at all time lows.&lt;/p&gt;

&lt;h3 id=&quot;wage-dispersions-part-in-the-productivity-puzzle&quot;&gt;Wage dispersion’s part in the productivity puzzle&lt;/h3&gt;

&lt;p&gt;Inequality and its effect on aggregate productivity is one broad problem in our economy. Another is stagnating wages. Even with declining productivity - at lows relative to a historical norm - real hourly compensation has not kept up. In all business cycles since 1970, hourly compensation has not kept pace with gains in labor productivity.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/bls_beyond_the_numbers.PNG&quot; /&gt;
	&lt;figcaption&gt;Analysis in Beyond the Numbers from BLS.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;This has also been a significant area of research, with the leading hypothesis &lt;a href=&quot;https://economics.mit.edu/files/12979&quot;&gt;that this is another product of the “superstar firm” effect&lt;/a&gt;. If technological changes or trade integration push sales towards the most productive firms in each industry, product market concentration will rise as industries become increasingly dominated by superstar firms, which have high markups and a low labor share of value-added. These &lt;a href=&quot;https://www.oecd.org/economy/outlook/Decoupling-of-wages-from-productivity-november-2018-OECD-economic-outlook-chapter.pdf&quot;&gt;trends affect wage dispersion&lt;/a&gt; (more negatively impacting low-skill workers):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Technology.&lt;/em&gt; Technological change appears to contribute to rising wage inequality. With given endowments of low and high-skilled labour (whose stock can be adjusted only slowly over time), technological change can raise wage inequality if it complements high-skilled workers but substitutes for low-skilled workers. Technological gains are also not equal between firms within an industry - contributing to an inequality of opportunity - which can be reflected in wages.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Trade Integration.&lt;/em&gt; Trade integration also appears to play a role in increased wage inequality. At the aggregate level, the ratio of median to average wages is negatively associated with value added imports. Evidence from micro-aggregated data further suggests that between-firm wage dispersion increased in sectors that became more open to trade (Berlingieri et al., 2017)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.focus-economics.com/blog/why-is-productivity-growth-so-low-23-economic-experts-weigh-in&quot;&gt;Dean Baker postulates&lt;/a&gt; that “…much of the story is endogenous in the sense that a weak labor market forces workers to take low pay and low productivity jobs. In fact, &lt;a href=&quot;https://www.bls.gov/opub/btn/volume-6/pdf/understanding-the-labor-productivity-and-compensation-gap.pdf&quot;&gt;adjusting labor wages for the price of the products they produce&lt;/a&gt; can account for a large portion of the wage gap. This does not imply that technological change and increased trade integration harm workers, &lt;a href=&quot;https://www.oecd.org/economy/outlook/Decoupling-of-wages-from-productivity-november-2018-OECD-economic-outlook-chapter.pdf&quot;&gt;since a large body of evidence suggests that these developments raise aggregate productivity&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;policy-redress-to-promote-a-well-diversified-economy&quot;&gt;Policy redress to promote a well-diversified economy&lt;/h3&gt;

&lt;p&gt;Over the last 30 years, the basis of competition has been changing and ICT has increasingly become a competitive advantage. More than the technology itself, “superstar firms” create intangible capital - e.g., the ability to organize themselves and design processes that becomes the basis for increased productivity. This should be celebrated!&lt;/p&gt;

&lt;p&gt;At the same time, it’s imperative that public policies support the broader creation and sharing of these opportunities. A few potential areas to start could be:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Greater investment in the “commons,”&lt;/strong&gt; which is &lt;a href=&quot;https://www.theatlantic.com/business/archive/2015/02/why-the-gap-between-worker-pay-and-productivity-is-so-problematic/385931/&quot;&gt;a set of shared resources&lt;/a&gt; that every business needs in order to be productive: an educated populace, pools of skilled labor, a vibrant network of suppliers, strong infrastructure, basic R&amp;amp;D and so on&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Restrengthening institutions and bargaining power&lt;/strong&gt;, which can promote the transmission of productivity gains to wages and typically takes place at the industry level. This may allow workers to appropriate industry-specific rents with minimal impact on capital-labour substitution&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Review tax and transfer systems&lt;/strong&gt; to account for an increase of gains to capital investment relative to labor investment&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reform in exit policies (e.g., insolvency regimes)&lt;/strong&gt; to support the assets and workers associated with low or unproductive firms&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Revised active labor market policies&lt;/strong&gt; to preserve the labour market attachment and skills of workers who may transition their jobs&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is important to consider that tradeoffs exist for policymakers when considering limiting the growth of large firms. We want to both celebrate success and support future growth. For example, on one hand, curbing the expansions of large firms can be detrimental to growth in the short run because large firms may be able to provide goods and services at a lower cost. However, curbing their expansions may also sustain profit margins that provide incentives for innovation and stimulate growth.&lt;/p&gt;

&lt;p&gt;My goal in putting this together was to clarify my own thinking - and to present it in a way that can be challenged and deepened by others. I would be grateful to learn what may resonate (or differ) from your personal perspectives and work.&lt;/p&gt;</content><author><name></name></author><summary type="html">A potential impact of unequal adoption of ICT.</summary></entry><entry><title type="html">The Digital Economy: Update.</title><link href="http://localhost:4000/digital-economy-review/" rel="alternate" type="text/html" title="The Digital Economy: Update." /><published>2019-12-20T00:00:00-08:00</published><updated>2019-12-20T00:00:00-08:00</updated><id>http://localhost:4000/digital-economy-review</id><content type="html" xml:base="http://localhost:4000/digital-economy-review/">&lt;p&gt;When I wrote my first articles on the digital economy in 2016, many economists were writing about a &lt;a href=&quot;https://www.newyorker.com/news/john-cassidy/the-great-productivity-puzzle&quot;&gt;“productivity puzzle.”&lt;/a&gt; Wage growth had been flat (in real terms) since 1970, productivity was at its lowest point since 1980, industry consolidation had been &lt;a href=&quot;https://www.richmondfed.org/-/media/richmondfedorg/publications/research/working_papers/2018/pdf/wp18-15.pdf&quot;&gt;monotonically rising since 1990&lt;/a&gt;, and industry dynamism was near its lowest point since 1975 (as measured by new firm entry and exit rates).&lt;/p&gt;

&lt;p&gt;Curious of what we may have learned in the last 3 years, I re-reviewed economic literature, policy papers, and investment recommendations to understand the impact of digital technologies on the economic growth of the United States. In recent years, economist have explored various supporting hypotheses, such as rising markups and market power (&lt;a href=&quot;https://www.nber.org/papers/w23583&quot;&gt;Gutierrez and Philippon, 2017&lt;/a&gt;; &lt;a href=&quot;http://www.janeeckhout.com/wp-content/uploads/RMP.pdf&quot;&gt;De Loecker, Eeckhout and Unger, 2018&lt;/a&gt;; Hall, 2018), the increasing profits of large firms (&lt;a href=&quot;https://www.gsb.stanford.edu/sites/gsb/files/jmp_simcha-barkai.pdf&quot;&gt;Barkai, 2017&lt;/a&gt;), declining labor market dynamism (&lt;a href=&quot;https://www.aeaweb.org/articles?id=10.1257/aer.p20171020&quot;&gt;Decker, Haltiwanger, Jarmin and Miranda, 2017&lt;/a&gt;), and declining wages and declining labor share (e.g. &lt;a href=&quot;https://www.nber.org/papers/w23396&quot;&gt;Autor, Dorn, Katz, Patterson, and Van Reenen, 2017&lt;/a&gt;). All said, &lt;strong&gt;the dynamics underlying the “productivity puzzle” are still prevalent and a continued area of research.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With that in mind, I led an inquiry to understand &lt;strong&gt;some of the more compelling explanations and/or challenges associated with these phenomena&lt;/strong&gt;, namely that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“Superstar firms” may be the strongest indicator of these dynamics: differentiated execution and more successful IT application leads to dispersion in wage growth, with disproportionate impact on low-skill workers&lt;/li&gt;
  &lt;li&gt;Through further automation, the divergence in economic outcomes will persist. The nature of automation and any associated job loss will be industry-specific&lt;/li&gt;
  &lt;li&gt;Workforce development and reskilling remain important for creating a dynamic economy; however, ambiguity in credentialing and role definition make skill development outside of the firm a challenging task&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In doing this review, I became increasingly confident that strengthening the US labor force is pressing for the general health and welfare of the economy, and each of these topics includes a review of the current policy levers. Inequality is present - as a result of poor dynamism (lack of firm exit, sluggish development of new skills, lower mobility) and insufficient policy support for those negatively affected by the aggregate gains.&lt;/p&gt;

&lt;p&gt;At the same time, my goal in putting this together was to clarify my own thinking - and to present it in a way that can be challenged and deepened by others. I would be grateful to learn what may resonate (or differ) from your personal perspectives and work.&lt;/p&gt;</content><author><name></name></author><summary type="html">Exploring the last several years of research on productivity.</summary></entry><entry><title type="html">Agile Data Science: Don’t use the ‘D’ word.</title><link href="http://localhost:4000/developing-agile-data-scientists/" rel="alternate" type="text/html" title="Agile Data Science: Don't use the 'D' word." /><published>2019-06-07T00:00:00-07:00</published><updated>2019-06-07T00:00:00-07:00</updated><id>http://localhost:4000/developing-agile-data-scientists</id><content type="html" xml:base="http://localhost:4000/developing-agile-data-scientists/">&lt;p&gt;Even if your company has &lt;a href=&quot;https://multithreaded.stitchfix.com/blog/2019/03/11/FullStackDS-Generalists/&quot;&gt;full stack data scientists&lt;/a&gt;, data science is still ultimately a team sport. There are business stakeholders, product managers, designers, members of the infrastructure team - all of whom may have input, questions, and dependencies on the data scientist’s work. Each of these individual members likely have different interests, experiences, responsibilities, and patterns for problem solving.&lt;/p&gt;

&lt;p&gt;This diversity in perspective can be an asset, when harnessed effectively. However, if not actively managed, these perspectives can also be a huge source of miscommunication, misalignment, lack of trust, and frustration. Acknowledging this, I discourage the word “done” from my projects. Far too often, the phrase “it’s done” is accepted - when a quick review between the two parties will reveal different expectations about what “done” is supposed to mean. For example, if your data scientist says, “the model is done / ready,” do they mean:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The code accurately describes the problem, and is written purely in scripts, or&lt;/li&gt;
  &lt;li&gt;The code is functional and parametric, and accounts for common edge cases, or&lt;/li&gt;
  &lt;li&gt;The code documented and tested, in a way that can transfer ownership, or&lt;/li&gt;
  &lt;li&gt;The code is schedulable and optimized, in a way that can run in production&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Depending on where the project is in development - any of these answers could be sufficient. The challenge is when differences exist between how the two parties interpret the message.&lt;/p&gt;

&lt;h5 id=&quot;insert-image-here&quot;&gt;&lt;strong&gt;Insert image here&lt;/strong&gt;&lt;/h5&gt;

&lt;h3 id=&quot;best-practices-on-removing-the-word-done-from-your-projects&quot;&gt;Best practices on removing the word “done” from your projects&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Empower the end user to own the logic:&lt;/em&gt; Sometimes, business users will treat the work of the data scientist as “magic.” This serves nobody - the model is not truly a “black box.” The code that dictates the inputs, the outputs, and the algorithmic approach is all logic. Sometimes the inference for a local prediction can’t be explained directly; that is a different topic than the overall problem design. Encourage conversation.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Educate the end user on the work of model building and management:&lt;/em&gt; There is &lt;a href=&quot;https://www.oreilly.com/radar/lessons-learned-turning-machine-learning-models-into-real-products-and-services/&quot;&gt;a lot of work&lt;/a&gt; to put models into production, and to maintain them once there - for example, with regard to drift, retraining, training/serving skew, security, maintenance, etc. End users should know they are investing in developing a system; the model is an artifact, not an end product, of that system.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Foster a learning culture and growth mindsets:&lt;/em&gt; Most parties working on data science projects are on a learning journey together. There is a significant amount that is “new” - most data scientists learn through apprenticeship – which can create challenges for how to communicate effectively. Simply acknowledging this can make an important difference.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Clearly articulate the tradeoffs with “good enough analysis”:&lt;/em&gt; In the data science workflow, it is easy to get stuck in a variety of local optima. For example, repeatedly asking for ad hoc analyses may give a greater intuition for the problem, while never giving the data scientist the time to professionalize her code. On the other hand, pushing for automation of a system may lead to too narrowly specifying the problem, eroding potential value. With software, there are real tradeoffs between flexibility and automation - and it is important to consider the appropriate balance for the system being defined.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When considering “done”, a large risk present in a “good enough analysis” culture is the potential to create technical debt. Technical debt often goes unrecognized until it is too late, &lt;a href=&quot;https://martinfowler.com/articles/is-quality-worth-cost.html&quot;&gt;creating unacknowledged costs&lt;/a&gt; that will eventually need to be paid down. Even with a high prevalence of ad hoc work, it is not an excuse for running an “unclean” process. Below, please find a few recommendations to “keep things clean.”&lt;/p&gt;

&lt;h5 id=&quot;tips-for-cleanliness-in-development&quot;&gt;&lt;em&gt;Tips for cleanliness in development:&lt;/em&gt;&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Use code spikes:&lt;/em&gt; &lt;a href=&quot;https://www.scaledagileframework.com/spikes/&quot;&gt;Code spikes&lt;/a&gt; are a term from XP Programming that refer to code used to understand a problem rather than develop working code. For data science, I treat EDA as the same - and keep it out of /src within the codebase.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Enforce a good PR process:&lt;/em&gt; Google has a &lt;a href=&quot;https://google.github.io/eng-practices/review/reviewer/&quot;&gt;wonderful guide for code reviewers&lt;/a&gt;. Key quote for me is, “Don’t accept PRs that degrade the code health of the system. Most systems become complex through many small changes that add up, so it’s important to prevent even small complexities in new changes.”&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Refactor and redesign as necessary:&lt;/em&gt; Clean code really pays off, and one should restructure the codebase as the problem definition changes. Here, it’s helpful to think about Martin Fowler’s &lt;a href=&quot;https://martinfowler.com/bliki/Yagni.html&quot;&gt;YAGNI&lt;/a&gt; and &lt;a href=&quot;https://softwareengineering.stackexchange.com/questions/197363/reasoning-to-wait-until-third-time-in-the-rule-of-three&quot;&gt;Rule of Three&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;tips-for-cleanliness-in-sprint-planning&quot;&gt;&lt;em&gt;Tips for cleanliness in sprint planning:&lt;/em&gt;&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Ensure Sprint Goals are output driven:&lt;/em&gt; Writing Sprint Goals are a bit of an art, needing to be both ambitious and achievable. As data scientists practice goal setting, watch out for task-oriented outcomes that make acceptance criteria less valuable and can lead to reduced productivity.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Limit User Stories to 1-2 story points:&lt;/em&gt; Remember that story points are used to size work during sprints. They aren’t a tool to measure a developer’s productivity, and should not shouldn’t bleed across sprints.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Create viable “Plan Bs”:&lt;/em&gt; Creating options is what makes Agile work work well. When building out “Agile tests” (&lt;a href=&quot;https://bradaallen.github.io/data-science-and-agile/&quot;&gt;prior post here&lt;/a&gt;), it is important to have credible alternatives for a path forward. Otherwise, the team won’t be managing risk - they’ll be rationalizing themselves towards a predefined outcome.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;good-luck&quot;&gt;Good luck!&lt;/h3&gt;

&lt;p&gt;Hopefully after reading this, you’ll agree to the danger of using the word “done” in cross-functional teams. Fortunately, with awareness, there are many things that teams can do - in terms of both culture and process - that foster innovation and improve the likelihood of success in data science initiatives.&lt;/p&gt;</content><author><name></name></author><summary type="html">Never say you're 'done'!</summary></entry><entry><title type="html">Data Science needs Agile and Product Management.</title><link href="http://localhost:4000/data-science-and-agile/" rel="alternate" type="text/html" title="Data Science needs Agile and Product Management." /><published>2019-04-18T00:00:00-07:00</published><updated>2019-04-18T00:00:00-07:00</updated><id>http://localhost:4000/data-science-and-agile</id><content type="html" xml:base="http://localhost:4000/data-science-and-agile/">&lt;p&gt;Over the last 5 years, I have led directly or been involved in a number of enterprise projects to introduce or expand the use of machine learning. As a Product Manager turned Data Scientist, I have found time and time again that many concepts in Product Management and Agile development help de-risk and enhance the overall effectiveness of ML solutions.&lt;/p&gt;

&lt;p&gt;In this article, I’ll highlight two key concepts - one from Agile and one from Product Management - and explain how they help build sustainable, reproducible ML systems.&lt;/p&gt;

&lt;h3 id=&quot;1-agile-planning-is-about-managing-uncertainty---you-are-trading-resources-eg-time-for-information-data-science-explorations-adopt-this-mode-of-thinking-well&quot;&gt;1. Agile planning is about managing uncertainty - you are trading resources (eg, time) for information. Data science explorations adopt this mode of thinking well.&lt;/h3&gt;

&lt;p&gt;Acknowledging uncertainty is the key differentiator from an Agile and a waterfall process. The only reason I might need to adjust my backlog or iterate my sprint plans is due to the fact that my original plan could be wrong. Things &lt;strong&gt;where being wrong hurts the most&lt;/strong&gt; should be (1) broken down into tests that (2) generate information that (3) allow for decision-making under greater certainty.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“If it hurts, do it more frequently, and bring the pain forward.” - Jez Humble&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In my projects, we call this aspect of project design “bringing the pain forward.” This is a concept from Jez Humble’s book on &lt;a href=&quot;https://www.amazon.com/Continuous-Delivery-Deployment-Automation-Addison-Wesley/dp/0321601912&quot;&gt;Continuous Delivery&lt;/a&gt;. His book focuses on automating deployment pipelines (testing, environment management, builds, etc.) to allow for more frequent pushes to production. However, the concept translates to more general risk management as well.&lt;/p&gt;

&lt;p&gt;A primary goal is to eradicate &lt;a href=&quot;https://landing.google.com/sre/sre-book/chapters/eliminating-toil/&quot;&gt;“toil”&lt;/a&gt; - work that doesn’t make a meaningful contribution to the final product. Agile planning for ML models and applications follows a similar logic - seek to eradicate incorrect lines of inquiry, model choices, UI, and technical design as quickly as possible.&lt;/p&gt;

&lt;p&gt;Aspects of the plan that are not tested are assumptions. Erroneous assumptions can be very dangerous. If I assume a fact without designing a test, I am propagating the risk that I could be wrong “throughout” the project.&lt;/p&gt;

&lt;p&gt;Even benign assumptions can kill the value proposition of a solution, so it’s important to treat them with respect. For example:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You do a small scale POC on one country’s transaction data using pandas or R. You know that scaling may require a centralized architecture and a distributed system, but you don’t have this discussion until you’ve proven your toy model’s efficacy. In discussing next steps, it becomes apparent your “value” is theoretical - the technical resources do not exist.&lt;/li&gt;
  &lt;li&gt;You are working to optimize the pricing of in-store goods. Designing an A/B test is difficult - you have a policy of consistent promotions (eg, no holdout) within a state or country to manage against &lt;a href=&quot;https://risnews.com/promotional-pricing-right-side-law&quot;&gt;complicated promotional pricing regulations&lt;/a&gt;. You attempt to &lt;a href=&quot;https://arxiv.org/pdf/1610.07748.pdf&quot;&gt;design a difference-in-differences or synthetic control method&lt;/a&gt; to prove incrementality, but the business sponsor dismisses it as theoretical. You have a conviction you’ve created value, but challenges in proving it.&lt;/li&gt;
  &lt;li&gt;You are working on a classification problem, e.g., churn. The business user has sold a UI in which every customer has accurate probability predictions and clear rationale supporting each customer’s prediction. However, &lt;a href=&quot;https://www.svds.com/learning-imbalanced-classes/&quot;&gt;low incidence rates (~1%) make probability calibration a challenge&lt;/a&gt;, and the range and nature of churn reasons makes &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/shap.html&quot;&gt;feature engineering for useful SHAP values&lt;/a&gt; costly and difficult. Had you been involved earlier, you would focus on ranking/discrimination and decoupling rationale from prediction, if possible.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the beginning of a ML exercise, make a list of everything that can go wrong - and use that as a starting point for your planning!&lt;/p&gt;

&lt;h3 id=&quot;2-data-science-plans-and-assumptions-are-best-managed-when-framed-around-a-set-of-product-and-technical-requirements&quot;&gt;2. Data science plans and assumptions are best managed when framed around a set of product and technical requirements.&lt;/h3&gt;

&lt;p&gt;Notice above that I mentioned Agile tests are for &lt;em&gt;“where being wrong matters most.”&lt;/em&gt; Experience and judgement really can make the difference between a successful project that completes on time and something that never quite works. Part of leading successful teams requires &lt;a href=&quot;https://blog.getdbt.com/4-questions-to-help-you-more-accurately-scope-analytics-engineering-projects/&quot;&gt;building reserves of trust&lt;/a&gt; with your team, your partners, and stakeholders.&lt;/p&gt;

&lt;p&gt;A good way to build trust is to give a clear conception of the end solution, with a draft set of product and technical requirements. It should be clear that these requirements are assumptions and subject to revision. It is helpful to communicate which requirements have greater uncertainty, and their associated tests in the plan.&lt;/p&gt;

&lt;p&gt;For example, at the beginning of a model and/or application’s development, I may not know:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Any &lt;strong&gt;required changes to operational processes&lt;/strong&gt; necessary for adoption (and the associated resistance to change)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Business constraints&lt;/strong&gt; that shape the necessary inputs/outputs - for example, pricing ladders constrain price outcomes, data loads/availability can affect scoring cadences&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;right form factor&lt;/strong&gt; that will ensure a tool is used&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;preferred business metrics&lt;/strong&gt; for measurement&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By having a solution-mindset, and by trying to define clear product and technical requirements, I can then build slices of functionality that allow me to iterate towards a final solution.&lt;/p&gt;

&lt;p&gt;A useful mental model for how to decompose a problem is shown by the &lt;a href=&quot;https://www.hackster.io/news/starting-with-one-1f0ab62cbed4&quot;&gt;“looks like” vs. “works like”&lt;/a&gt; approach to prototyping. In this approach, the UI/UX of a solution (the “looks like”) and its core functionality (the “works like”) can be developed separately and in parallel. As feedback about each component is collected, it may influence the requirements of the other component. For example, technical limitations may impact requirements for the user experience, and revisions of the user workflow may change the roadmap/priorities of the different technical features.&lt;/p&gt;

&lt;p&gt;Each component of the overall solution can then be further decomposed into basic components, with a priority for collecting quick feedback. This follows John Gall’s famous &lt;a href=&quot;https://quotesondesign.com/john-gall/&quot;&gt;quote&lt;/a&gt;: &lt;em&gt;“A complex system that works is invariably found to have evolved from a simple system that worked.”&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;a-few-recommendations-for-building-out-your-ml-solution&quot;&gt;A few recommendations for building out your ML solution:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;For the solution design, the first simple system should be a &lt;a href=&quot;https://codeclimate.com/blog/kickstart-your-next-project-with-a-walking-skeleton/&quot;&gt;“walking skeleton.”&lt;/a&gt; Having a functional end-to-end pipeline with limited functionality will ferret out any integration issues early, and can surface necessary storage and processing patterns.&lt;/li&gt;
  &lt;li&gt;For the model design, the first simple system should be the quickest MVP that creates a baseline prediction. &lt;a href=&quot;https://techdevguide.withgoogle.com/resources/rules-of-ml/&quot;&gt;Google’s Rules of ML&lt;/a&gt; suggest using ML only after heuristics become too complex. This logic should extend to ML solutions as well, starting with straightforward implementations that become increasingly sophisticated (eg, start with classical time-series methods for forecasting before testing nonlinear approaches).&lt;/li&gt;
  &lt;li&gt;For interaction design, design thinking approaches should guide the first simple system:
    &lt;ul&gt;
      &lt;li&gt;Generate options for yourself - &lt;a href=&quot;https://www.teachthought.com/critical-thinking/3-modes-of-thought-divergent-convergent-thinking/&quot;&gt;get comfortable exploring ideas using divergent thinking&lt;/a&gt;. Great solutions often steal components from multiple good ideas.&lt;/li&gt;
      &lt;li&gt;Find ways to &lt;a href=&quot;https://www.wired.com/2014/02/stanford-class-taught-two-normal-guys-star-designers/&quot;&gt;collect user behavior&lt;/a&gt; - information through engagement can often be of higher quality than basic market or user research.&lt;/li&gt;
      &lt;li&gt;Get engagement through visual tools (eg, &lt;a href=&quot;https://www.experienceux.co.uk/faqs/what-is-wireframing/&quot;&gt;wireframing&lt;/a&gt;, excel mockups) - take time to listen and pay attention to points of friction or counterintuitive (to use) usage&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;applied-ml---managing-the-high-risk-high-reward-tradeoff&quot;&gt;Applied ML - managing the “high risk, high reward” tradeoff.&lt;/h3&gt;

&lt;p&gt;ML systems are known for the tremendous value they can create, as well as their potential for &lt;a href=&quot;https://research.google/pubs/pub43146/&quot;&gt;“high interest” technical debt&lt;/a&gt;, drift, biased predictions, etc. These challenges are often coupled with the need to educate business users on how to interact with such systems, their role and responsibilities, and managing expectations about what can vs. what cannot be done.&lt;/p&gt;

&lt;p&gt;Proven software development approaches for doing this well - particularly Agile and Product Management principles - can create trust and accelerate the adoption of these tools and capabilities.&lt;/p&gt;</content><author><name></name></author><summary type="html">Or, 'how to manage expectations under uncertainty.'</summary></entry><entry><title type="html">Review of recent trends in Data Science.</title><link href="http://localhost:4000/pulse-on-data-science-trends/" rel="alternate" type="text/html" title="Review of recent trends in Data Science." /><published>2019-02-21T00:00:00-08:00</published><updated>2019-02-21T00:00:00-08:00</updated><id>http://localhost:4000/pulse-on-data-science-trends</id><content type="html" xml:base="http://localhost:4000/pulse-on-data-science-trends/">&lt;p&gt;&lt;em&gt;Every week, I spend ~5h reading my favorite newsletters on data science (eg, Data Science Roundup, Data Elixir, Jack Clark, Daniel Meissner). My most recent project prevented me from catching up on these articles for a few months - so I took a few days to catch up and synthesize what I had been seeing. That summary can be found below.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;key-takeaways&quot;&gt;Key takeaways&lt;/h3&gt;

&lt;p&gt;Most of the interesting articles were saying the same things:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Role definition is still happening, but there is alignment that hybrid models in business are useful&lt;/li&gt;
  &lt;li&gt;Businesses are continuing to invest in these initiatives, more understanding that it is not “magic”&lt;/li&gt;
  &lt;li&gt;Platforms and cloud tooling continue to enforce the imperative for DS to be able to own models “end to end” - the DS needs to write clean code and launch to production; can be on platforms built by SWE &amp;amp; DE&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I didn’t see very much interesting on actual techniques (&lt;a href=&quot;https://www.basilica.ai/blog/the-unreasonable-effectiveness-of-deep-feature-extraction/&quot;&gt;besides a very interesting post on transfer learning as deep feature extraction&lt;/a&gt;), but have shared some below. Lots of talk on ethics, some talk on RL and all of the NLP stuff (Bert &amp;amp; ELMo, etc – great summary of this as an ImageNet moment &lt;a href=&quot;https://thegradient.pub/nlp-imagenet&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;role-of-a-data-engineer&quot;&gt;Role of a Data Engineer&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://blog.fishtownanalytics.com/does-my-startup-data-team-need-a-data-engineer-b6f4d68d7da9&quot;&gt;Do I need a data engineer?&lt;/a&gt; Here is a second article, &lt;a href=&quot;https://www.locallyoptimistic.com/post/analytics-engineer/&quot;&gt;distinguishing between DE and an “Analytics Engineer.”&lt;/a&gt; Suggests using stitch, fivetran, dbt as data engineering tools in lieu of Airflow. Natural migration (in startups at least) is to do PoCs, early builds on Airflow and then migrate to a more resilient tool like those listed. Super critical role (and primary responsibilities of our principal data engineers on a project), should be responsible for:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Managing and optimizing core data infrastructure,&lt;/li&gt;
  &lt;li&gt;Building and maintaining custom ingestion pipelines,&lt;/li&gt;
  &lt;li&gt;Supporting data team resources with design and performance optimization (think 1 DE for 3 DS) and&lt;/li&gt;
  &lt;li&gt;Building non-SQL transformation pipelines (PySpark ETL (maybe), geo enrichment)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I thought the idea of removing Airflow for SQL transformations was an interesting trend. I haven’t ever used the three “pipeline-as-a-service” products. For my own projects, the above responsibilities were good to have for our primary data engineer, with a separate, proper SW developer as the code master.&lt;/p&gt;

&lt;h3 id=&quot;more-on-role-definition-the-kinds-of-a-data-scientist&quot;&gt;More on role definition. &lt;a href=&quot;https://hbr.org/2018/11/the-kinds-of-data-scientist&quot;&gt;“The Kinds of a Data Scientist.”&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;The VP, DS of Instacart &lt;a href=&quot;https://firstround.com/review/doing-data-science-right-your-most-common-questions-answered/&quot;&gt;split the key types of work&lt;/a&gt; into “Decision Science” vs. “Data Products” to identify skills required. I thought the Decision Science example was pretty interesting.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;*At LinkedIn, the executive team used decision science to make a critical business decision about the visibility of member profiles in search results. Historically, only paid users could see full profiles for everyone in their extended (third-degree) network. The visibility rules were complex, and LinkedIn wanted to simplify them — but not in a way that would undermine its revenue. The stakes were enormous.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;The proposed visibility model was a monthly use limit for unpaid users, with a cut-off based on usage. LinkedIn’s decision scientists simulated the effects of this change, using historical behavior to predict the impact on revenue and engagement. The analysis had to extrapolate past behavior on one model to forecast behavior on a radically different one. Nonetheless, the analysis was sufficient to move forward.*&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.locallyoptimistic.com/post/code-as-configuration/&quot;&gt;“Code as Configuration”&lt;/a&gt;. Article written by an experienced DS outlining how DS &amp;amp; SWE/DE should think about working together.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;project-management&quot;&gt;Project management.&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackernoon.com/rethinking-fast-and-slow-in-data-science-b2ce18d5b054&quot;&gt;Doing Agile in Data Science&lt;/a&gt;. Google. Particular focus on taking the scientific method and breaking it down into “less rigorous hypotheses” that can be (1) time bound and (2) better inform the LT hypotheses you want to prove or disprove in an experiment.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://simplystatistics.org/2018/09/14/divergent-and-convergent-phases-of-data-analysis/&quot;&gt;Divergent and convergent thinking in data analysis&lt;/a&gt;. An okay article, on a great concept. With the article above, how do we coach projects to (1) generate good hypotheses with the associated code “spikes”, (2) then fit that into design patterns, etc. that allow us to grow code safely? This can also be very useful for identifying product/project/pilot requirements with business users. I’ve received feedback in projects that “divergent thinking is very uncomfortable” but the decisions it can lead to are almost always of higher value than what a convergent path would provide.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=qCKj_K5RNfY&quot;&gt;Design for Continuous Experimentation&lt;/a&gt;. Etsy. How Etsy learned to build stage gates into their product development process and use A/B tests to enable stage gate decisions (!!!) I liked how the Principal Engineer framed the way they used their learning from things that did not go well for future projects&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/coursera-engineering/should-engineering-managers-write-code-wrong-question-ec5fc54d3903&quot;&gt;How much should managers code? Wrong question. Where to write code?&lt;/a&gt; Coursera. Emphasizes being invested in small bug fixes, code reviews, and JIRA to develop a manager’s empathy for the team’s work and foster better outcomes.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.insightdatascience.com/an-introduction-to-the-data-product-management-landscape-ef930afe6de5&quot;&gt;The State of Data Product Management Roles&lt;/a&gt;. Insight Data Science. Highlights 5 domain areas: Infrastructure, Analytics, Applied ML/AI, Platforms, Standardization &amp;amp; Discovery. Note that Analytics and Applied AI/ML roles map well to the two different “kinds of data scientists” outlined above, and the other 3 are DevOps-y in nature.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experimentation&quot;&gt;Experimentation.&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://hookedondata.org/guidelines-for-ab-testing&quot;&gt;Guidelines for AB Testing at Etsy&lt;/a&gt;. Advocates for frequentists methods and to establish measurement up front first. Great anecdote on how more measurement up front changed their delivery process for new features. Many, many links for follow on reads.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://varianceexplained.org/r/bayesian-ab-testing/&quot;&gt;Is Bayesian Testing Immune to Peeking? Not Exactly&lt;/a&gt;. Stack Overflow (the company). Less about the Bayesian approach, I thought this was a great way of showing how to use simulation to trust the data behind your decisions (and how p-values can change over time). I fell into a small rabbit hole reading about AB testing from these articles; a theme for me was many different teams using simulation to “shore up” questions around experiment design.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://eng.uber.com/analyzing-experiment-outcomes/&quot;&gt;Analyzing Experiment Outcomes: Beyond ATEs&lt;/a&gt;. Uber. Walks through a metric called Quantile Treatment Effect (QTE) that allows for understanding heterogeneity in treatment effects. Cool.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/airbnb-engineering/experimentation-measurement-for-search-engine-optimization-b64136629760&quot;&gt;Experimentation &amp;amp; Measurement for SEO&lt;/a&gt;. Airbnb. Useful case study of when to use differences-in-differences and some of the idiosyncrasies to account for.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data--infrastructure-engineering&quot;&gt;Data / infrastructure engineering.&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/airbnb-engineering/capturing-data-evolution-in-a-service-oriented-architecture-72f7c643ee6f&quot;&gt;Capturing data evolution in a SOA&lt;/a&gt;. Airbnb&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://multithreaded.stitchfix.com/blog/2018/09/05/datahighway/&quot;&gt;Putting the power of Kafka into the hands of data scientists&lt;/a&gt;. Stitch Fix&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.oreilly.com/ideas/the-future-of-data-warehousing&quot;&gt;Future of Data Warehousing&lt;/a&gt;. Cloudera/PNC. Short, 5 min video outlining some challenges standing up a distributed system – seen these challenges at all projects and would expect our DE to work with and help the client set up these processes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;techniques-tools--interesting-reads&quot;&gt;Techniques, Tools, &amp;amp; Interesting reads.&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://multithreaded.stitchfix.com/blog/2018/11/08/bandits/&quot;&gt;Contextual bandits for content outreach&lt;/a&gt;. Stitch Fix&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://robatwilliams.github.io/decent-code/&quot;&gt;How to write clean code and conduct code reviews&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://exxeta.github.io/2018/10/forecast_sales_in_retail&quot;&gt;Forecast sales in retail&lt;/a&gt;. A good reference for establishing a baseline and packages/methods&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/3-facts-about-time-series-forecasting-that-surprise-experienced-machine-learning-practitioners-69c18ee89387&quot;&gt;3 facts about time-series analyses that can surprise ML people&lt;/a&gt;. (eg, “The uncertainty of the forecast is just as important as, or even more so, than the forecast itself.”)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/&quot;&gt;11 classical time series forecasting methods in Python&lt;/a&gt;. Related: &lt;a href=&quot;https://eng.uber.com/forecasting-introduction/&quot;&gt;Forecasting at Uber&lt;/a&gt;. Interesting to see Uber’s built-in backtesting system.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rise.cs.berkeley.edu/blog/a-short-history-of-prediction-serving-systems/&quot;&gt;A short history of prediction serving systems&lt;/a&gt;. RISE Lab. Discusses the architecture of RISE’s Clipper system, and has an interesting paper from LinkedIn on managing the tradeoff between accuracy and serving latency.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/nteract/papermill#usage&quot;&gt;Papermill&lt;/a&gt;. Parametric, scheduled Jupyter notebooks with the option for aggregate summaries. Used a lot at Netflix.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://better.engineering/convoys/&quot;&gt;Convoys&lt;/a&gt;. PyPI package for time-lagged conversion rates (eg, telco churn..)&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Topics: data engineering, role definition, project management, experimentation, et al.</summary></entry><entry><title type="html">Is Your Customer Journey Set Up for Success? Part II.</title><link href="http://localhost:4000/customer-journey-success-part-2/" rel="alternate" type="text/html" title="Is Your Customer Journey Set Up for Success? Part II." /><published>2018-07-10T00:00:00-07:00</published><updated>2018-07-10T00:00:00-07:00</updated><id>http://localhost:4000/customer-journey-success-part-2</id><content type="html" xml:base="http://localhost:4000/customer-journey-success-part-2/">&lt;p&gt;&lt;em&gt;This article was written in 2017 during my time at SVDS. The article was first published on the &lt;a href=&quot;https://www.svds.com/customer-journey-set-success/&quot;&gt;SVDS Blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In the &lt;a href=&quot;https://bradaallen.github.io/customer-journey-success-part-1/&quot;&gt;introductory post&lt;/a&gt;, we walked through some examples of how SVDS has seen data capabilities determine the success of customer journey initiatives for our clients. In this post, we offer guidance on the data-related initiatives that you can start today to begin fostering closer ties with your customers—regardless of where you currently are in your specific state of development.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;customer-journeys-allow-for-a-holistic-experience&quot;&gt;Customer journeys allow for a holistic experience&lt;/h3&gt;

&lt;p&gt;A senior marketer’s ability to find and create valuable experiences for customers has grown dramatically in recent years. Beyond the traditional responsibilities of brand and creative management, senior marketers (such as CMOs, Brand Managers, and Product Marketing Managers) now use analytics to track customer interactions, measure the quality of engagement, and determine the effectiveness of an enormous range of different marketing tactics.&lt;/p&gt;

&lt;p&gt;Marketers often map out a &lt;a href=&quot;https://hbr.org/2015/11/competing-on-customer-journeys&quot;&gt;“customer journey”&lt;/a&gt; in order to manage successful engagements. The customer journey is the complete sum of experiences that your customers go through when interacting with your company and brand—mapping out these interactions gives you a holistic view of how customers engage with your company. While many marketers focus on developing positive interactions, a customer journey is a plan that focuses on how a series of engagements can generate momentum from awareness, to sale, to ongoing loyalty and advocacy.&lt;/p&gt;

&lt;h3 id=&quot;creating-customer-journey-opportunities-with-data&quot;&gt;Creating customer journey opportunities with data&lt;/h3&gt;

&lt;p&gt;At SVDS, we have seen leaders use data to drive more useful customer engagements—first and foremost by recognizing their need to embrace change. There is a big shift taking place, and that shift will become the new normal.&lt;/p&gt;

&lt;p&gt;You should be trying to learn quickly, and fail fast. Companies that are able make the best use of their data and infrastructure earlier than their competition are at an advantage—both with regard to increased customer loyalty and improvements in new product development.&lt;/p&gt;

&lt;p&gt;In a &lt;a href=&quot;https://bradaallen.github.io/customer-journey-success-part-1/&quot;&gt;November 2015 Harvard Business Review article&lt;/a&gt; on the customer journey, the authors stated that, “Best practitioners aim not just to improve the existing journey but to expand it, adding useful steps or features.” As mentioned in the examples earlier in this post, we have seen the same trend: clients who can harness their data to create effective customer experiences often make further investments toward developing their capabilities.&lt;/p&gt;

&lt;p&gt;There are data-related initiatives that you can begin pursuing today to develop stronger relationships with your customers. Make an honest assessment of where you stand now, and find yourself in the sections below.&lt;/p&gt;

&lt;h4 id=&quot;if-you-are-starting-from-scratch&quot;&gt;If you are starting from scratch:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Common Challenges&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Redefining decision-making based on insights&lt;/li&gt;
      &lt;li&gt;Identifying single points of ownership&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Common Solutions&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Focus on top-down buy-in. Without recognition from leadership, data initiatives will struggle to get relevance with business users and may be piecemeal efforts, diminishing the value of investment.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;if-you-are-performing-early-project-identification&quot;&gt;If you are performing early project identification:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Common Challenges&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Mapping out customer journey initiatives&lt;/li&gt;
      &lt;li&gt;Identifying single points of ownership&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Common Solutions&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Get alignment on the full picture. It is important to be able to articulate the “full view” of the customer experience as it has a large effect on decision making. For example, only tracking successful interactions would lead to very different conclusions than understanding users that “turn away.”&lt;/li&gt;
      &lt;li&gt;Plan for iteration. It often takes time to understand where your map does and does not match your customers’ realities.&lt;/li&gt;
      &lt;li&gt;Start small. Change can be incremental—look for low-hanging fruit.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;if-you-are-establishing-customer-visibility&quot;&gt;If you are establishing customer visibility:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Common Challenges&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Data integration&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Common Solutions&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Prioritize data collection in tandem with journey mapping. In instrumentation, it is important to know what behaviors you can collect directly from customers and what behaviors you have to infer based on their actions. This influences what additional data sources you include to support decisions you make for the business.&lt;/li&gt;
      &lt;li&gt;Seek to reuse and extend data services. Developing known, validated, and consistent data assets for your business increases their utility and dramatically improves trust in the developed insights across the organization.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;if-you-are-optimizing-for-growth&quot;&gt;If you are optimizing for growth:&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Common Challenges&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Personalization and automation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Common Solutions&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Enable advanced analytics. Optimize for automation to create feedback loops and self-learning capabilities that make it easy to identify and capitalize on growth opportunities.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What steps are you taking to strengthen your customer engagement strategies through better use of data?&lt;/p&gt;</content><author><name></name></author><summary type="html">Practical recommendations for any level of sophistication.</summary></entry><entry><title type="html">Is Your Customer Journey Set Up for Success? Part I.</title><link href="http://localhost:4000/customer-journey-success-part-1/" rel="alternate" type="text/html" title="Is Your Customer Journey Set Up for Success? Part I." /><published>2018-07-03T00:00:00-07:00</published><updated>2018-07-03T00:00:00-07:00</updated><id>http://localhost:4000/customer-journey-success-part-1</id><content type="html" xml:base="http://localhost:4000/customer-journey-success-part-1/">&lt;p&gt;&lt;em&gt;This article was written in 2017 during my time at SVDS. The article was first published on the &lt;a href=&quot;https://www.svds.com/customer-journey-set-success/&quot;&gt;SVDS Blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;customer-journeys-allow-for-a-holistic-experience&quot;&gt;Customer journeys allow for a holistic experience&lt;/h3&gt;

&lt;p&gt;A senior marketer’s ability to find and create valuable experiences for customers has grown dramatically in recent years. Beyond the traditional responsibilities of brand and creative management, senior marketers (such as CMOs, Brand Managers, and Product Marketing Managers) now use analytics to track customer interactions, measure the quality of engagement, and determine the effectiveness of an enormous range of different marketing tactics.&lt;/p&gt;

&lt;p&gt;Marketers often map out a &lt;a href=&quot;https://www.forrester.com/Customer-Journey&quot;&gt;“customer journey”&lt;/a&gt; in order to manage successful engagements. The customer journey is the complete sum of experiences that your customers go through when interacting with your company and brand—mapping out these interactions gives you a holistic view of how customers engage with your company. While many marketers focus on developing positive interactions, a customer journey is a plan that focuses on how a series of engagements can generate momentum from awareness, to sale, to ongoing loyalty and advocacy.&lt;/p&gt;

&lt;p&gt;According to a &lt;a href=&quot;https://www.salesforce.com/blog/2014/11/new-report-salesforce-marketing-cloud-linkedin-state-marketing-leadership.html&quot;&gt;Salesforce report&lt;/a&gt;, nearly all senior level marketers agree that a comprehensive journey map is absolutely critical or very important to their business. At the same time, the report mentions that this map has largely been an aspiration for marketers—in part due to siloed business teams and a disjointed view of customer data, only 29% of enterprise companies would rate themselves as very effective or effective at creating a cohesive journey.&lt;/p&gt;

&lt;p&gt;Senior marketers should take the responsibility for this challenge head on; to be successful in creating a useful map, you will also need to be the leader of the technical and analytical development of their teams. You should have an intuition for how data can enhance, track, and articulate the customer experience—as this intuition creates new possibilities for the type of relationships companies can have with their customers.&lt;/p&gt;

&lt;p&gt;In this first post, we’ll walk through some examples of how we have seen data capabilities determine the success of customer journey initiatives for our clients. In &lt;a href=&quot;https://bradaallen.github.io/customer-journey-success-part-2/&quot;&gt;subsequent posts&lt;/a&gt;, we’ll also offer guidance on the data-related initiatives that you can start today to begin fostering closer ties with your customers—regardless of where you currently are in your specific state of development.&lt;/p&gt;

&lt;h3 id=&quot;what-can-your-data-do-for-you&quot;&gt;What can your data do for you?&lt;/h3&gt;

&lt;p&gt;We’ve seen data play various roles in creating strong customer engagements. Here’s a look at just a few.&lt;/p&gt;

&lt;h4 id=&quot;optimize-against-competing-outcomes-using-effectively-stitched-data-social-gaming&quot;&gt;Optimize against competing outcomes using effectively stitched data (Social Gaming)&lt;/h4&gt;

&lt;p&gt;For many businesses, legacy and heterogeneous systems are a challenge for creating an integrated customer experience—the data is often structured with a narrow lens on a specific product or domain. We worked with a social gaming company facing this challenge: they wanted to create a cohesive customer experience across all their games by extending the preferential treatment that loyal customers receive for their favorite games to new games on the platform.&lt;/p&gt;

&lt;p&gt;On a per-game basis, our client was technically sophisticated—they could build out events, correlate performance with targeted marketing strategies, and articulate the effectiveness of different campaigns. However, this sophistication fell apart at the organization-wide level. After several acquisitions and the use of third party licensing for games, the company found itself with a broken analytical architecture—each game optimized for itself, but there was a lot of opportunity in optimizing across the business. By supporting the integration of different data from different silos, the new architecture enabled the company to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Increase customer satisfaction by rewarding loyalty.&lt;/em&gt; By not being able to establish a single view of their customers, our client was continually losing opportunities to tailor experiences for their customers. For example, high-paying customers that had “preferred” status for certain games would return to being “unknown” when they started playing new games. By helping the client carry customer status across games, players will have greater satisfaction and loyalty.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Improve game development by understanding interaction patterns.&lt;/em&gt; Some of the most important metrics in gaming—e.g., alliances and teaming—are challenging to measure. Breaking down silos allowed the client to utilize novel sources of data, like gaming chats, to articulate a “web of influence” and its role in engagement and profitability.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By reframing the customer journey as an experience that transcends individual games—and developing a supporting data architecture—our client was able to both develop an engine for growth and improve profitability on an individual basis, by both reducing the acquisition cost (UAC) of customers and increasing their lifetime value (LTV).&lt;/p&gt;

&lt;h4 id=&quot;redefine-your-brand-and-product-suite-with-a-more-modern-architecture-digital-entertainment&quot;&gt;Redefine your brand and product suite with a more modern architecture (Digital Entertainment)&lt;/h4&gt;

&lt;p&gt;Take a look at the data sources you’re using in your marketing efforts and you may find some unexpected insights. For example, is there untapped value in your existing customer relationships? A digital entertainment company wanted to develop a modern database architecture that would allow them to understand user consumption at both a customer and population-level—at a microsecond granularity.
Redesigning the client’s architecture to identify and articulate the customer’s consumption patterns ultimately gave the client vastly more usable data about their customers, which led to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;New customer offerings.&lt;/em&gt; Our client used these new capabilities to develop more effective cross-selling opportunities and to develop products that provide guidance to content providers.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Improved strategic decision making.&lt;/em&gt; This view of consumption informed large strategic bets for the organization—for example, the decision to give away existing products for free in return for increased engagement.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The client had always had tremendous potential to understand customer engagement and consumption patterns—they were a portal for millions of users. However, their existing platform was limited by its underlying technology and a myopic view of the role of data. In their original product offering, the collected data was not necessarily perceived to have inherent value. We find this type of oversight to be common for marketers beginning to build analytics within their teams.&lt;/p&gt;

&lt;p&gt;Note: This concept of “instrumentation”—the process of logging and tracking customer interactions—is important when creating an engagement plan. Instrumentation creates a more nuanced understanding of what customers find valuable about your products and services. For our client, this instrumentation influenced their entire business: from feature development, to sales, to pricing, and even to marketing copy about the efficacy of their existing product suite. Instrumentation is so important, in fact, that it is something we at SVDS specifically assess when considering the &lt;a href=&quot;https://www.svds.com/understanding-your-data-maturity/&quot;&gt;data maturity of a business&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;update-back-office-processes-to-redefine-customer-expectations-national-retailer&quot;&gt;Update back-office processes to redefine customer expectations (National Retailer)&lt;/h4&gt;

&lt;p&gt;Work that begins in the marketing department often extends to influence other parts of a business. In one example, we worked with a traditional brick and mortar retail client that was in the process of developing their digital presence.&lt;/p&gt;

&lt;p&gt;As their customers began to spend more time purchasing items online, our client realized that they would need new capabilities to support new types of interactions—for example, granularity in tracking inventory. When creating its online presence, our client found itself frequently selling under-stocked items from the website, and then had to follow them up with costly gift card apologies to disappointed customers.&lt;/p&gt;

&lt;p&gt;We helped the client gain an understanding of their inventory baseline and establish a real-time view of changes in supply and demand. This allowed the business as a whole to establish new customer relationships and a leaner efficiency:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Customer interaction with the brand.&lt;/em&gt; E-commerce has developed some great features to incent purchasing behavior—we have all experienced messages like, “There are only 3 items left in stock!” or “Order by 11:59pm Tuesday to get by Christmas.” Our client’s new modern inventory infrastructure allowed them to create similar features, increasing trust and confidence in the brand.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Reduced working capital.&lt;/em&gt; In the past, a granular level of detail—across stores, channels, and partners (e.g., third party sellers)—was not required from legacy operations. In developing a new system, our client was able to serve items that were stocked-out in their web distribution centers from stores in close proximity to the buyers. This allowed our client to reduce their buffer stock and protect against obsolescence.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For our retail client, marketing’s strategic influence and digital leadership forced growth in other parts of the business that benefited customers and improved the company’s competitive position.&lt;/p&gt;

&lt;h3 id=&quot;next-steps&quot;&gt;Next Steps&lt;/h3&gt;

&lt;p&gt;How is your company taking steps to strengthen their customer engagement strategies through better use of data?&lt;/p&gt;</content><author><name></name></author><summary type="html">Case studies from high performing transformations.</summary></entry><entry><title type="html">Data Maturity: Technology is Not Enough</title><link href="http://localhost:4000/technology-isnt-everything/" rel="alternate" type="text/html" title="Data Maturity: Technology is Not Enough" /><published>2018-05-15T00:00:00-07:00</published><updated>2018-05-15T00:00:00-07:00</updated><id>http://localhost:4000/technology-isnt-everything</id><content type="html" xml:base="http://localhost:4000/technology-isnt-everything/">&lt;p&gt;&lt;em&gt;This article was co-authored along with Scott Kurth in 2017, based on our experience at SVDS. The article was first published on the &lt;a href=&quot;https://www.svds.com/value-centered-data-maturity/&quot;&gt;SVDS Blog&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;success-with-data-involves-managing-strengths-and-weaknesses&quot;&gt;Success with data involves managing strengths and weaknesses&lt;/h3&gt;

&lt;p&gt;The great data rush is well and truly under way. Across virtually every industry, companies large and small are committing serious money to standing up their data infrastructure, beefing up capabilities, and hunting for the value hidden in their data—but often without a clear plan. No wonder that many of the business leaders we speak with suspect their initiatives are underperforming. The complaint we hear most frequently, regardless of industry, is that technology investments aren’t generating the expected returns.&lt;/p&gt;

&lt;p&gt;In a &lt;a href=&quot;https://www.svds.com/first-steps-strategy/&quot;&gt;previous post on data maturity&lt;/a&gt;, we discussed a company that was just embarking on a transformation: launching a new services business and building data capabilities to support that business. But what if you’re not starting from the beginning? What if you’ve already been embracing new technology, conducting pilots, and launching new analytical platforms? Recently, we were working with a Fortune 500 industrial company in the midst of developing software services to improve product R&amp;amp;D and enrich the customer experience.&lt;/p&gt;

&lt;p&gt;Their goal was to &lt;a href=&quot;https://svds.com/building-a-data-driven-culture/&quot;&gt;use data to empower decision makers&lt;/a&gt; across every part of the organization to make robust, data-driven choices. The company had great talent, technical vision, and infrastructure. &lt;strong&gt;Still, they weren’t generating the progress they would have liked at a rate they would have expected. What was wrong?&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;sometimes-the-business-can-hold-good-technology-behind&quot;&gt;Sometimes, the business can hold good technology behind&lt;/h3&gt;

&lt;p&gt;Working with our client to articulate their overall data maturity shined a light on areas requiring attention to get the most of their technology investments:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Missing links between projects and metrics:&lt;/em&gt; The initiative’s overall success was being measured by a single metric that they could only begin tracking in 2020—at the completion of the transformation. This led to significant uncertainty within project teams building new capabilities and platforms. Many teams were unsure where their analytical work fit in the big into the larger efforts and, more importantly, whether they were contributing to the overall success of the transformation.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Lack of cross-functional teams:&lt;/em&gt; The analytical infrastructure built by the engineering team was impressive, but was sorely underutilized. The data scientists had not been trained to use it and did not know how to access it. We heard from an analytics manager: “Seventy percent of my team’s time is spent on writing UDFs and Pig scripts to access data!” Creating teams that facilitated collaboration between engineers and data scientists was an opportunity for quick productivity gains with expensive talent.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Siloed business functions:&lt;/em&gt; Teams felt a lack of clear objectives that stemmed from communication and information sharing issues with other teams. For example, one team integral to product development described their view of the future as “a dusty window.” Business units on the consumption side of application development experienced very uneven usage of analytical tools. Strengthening these partnerships was crucial as the overarching project’s success relied specifically on strong analytical capabilities throughout the entire organization.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In working together, we helped the industrial company link their data and analytical capabilities with their ultimate business objectives, allowing them to create the right metrics to truly understand their progress. We helped them improve their devops capabilities and better integrate their engineering and data science teams. Collectively, this helped them break down technical and organizational siloes that were hampering progress.&lt;/p&gt;

&lt;p&gt;Understanding the uneven maturity of their capabilities across people, process, and systems gave them the answers they needed to the question on everyone’s minds: How can we see real results faster? A view of the current state of maturity along with a clear roadmap for success creates a baseline and a way to measure progress.&lt;/p&gt;</content><author><name></name></author><summary type="html">Knowing what to do with it is what matters most.</summary></entry></feed>